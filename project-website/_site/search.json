[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Grace Hyland is a student at Georgetown University. She is currently working on a B.S. in Biology of Global Health and taking classes towards an M.S. in Data Science and Analytics through the accelerated BS/MS program.\n\n\n\nheadshot"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DSAN 5000 Project",
    "section": "",
    "text": "Grace Hyland is a student at Georgetown University. She is currently working on a B.S. in Biology of Global Health and taking classes towards an M.S. in Data Science and Analytics through the accelerated BS/MS program.\n\n\n\nheadshot"
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "DSAN 5000 Portfolio Project",
    "section": "",
    "text": "In recent decades, the time and attention put into understanding sports using data and modeling has greatly increased for all sports. The earliest adopting sport was baseball and some close followers have been American football and basketball. The sport of ice hockey took longer to adopt the practice relative to the others. One of the proposed reasons for this is that there are specific challenges that come with trying to collect and analyze data from hockey.\nThese challenges include the infrequency of scoring events, the continuity of game play, and frequent substitutions. Ice hockey is one of the lowest scoring sports, especially compared to the likes of football and basketball. It is not uncommon for games to have fewer than two or three total goals scored. Given this, it can be hard to quantify the impact of each player on the success of their team and the impact that an individual play can have on a game’s outcome without creating more advanced statistical evaluation tools. It is as a result of this fact that the game of hockey is rather continuous compared to the discrete nature of sports such as football and baseball. Without a high number of scoring events, play is much less frequently halted so it is more difficult to classify events as individual plays. Additionally, the lack of stoppages in play mean that players are substituted in the middle of plays and events. This in combination with the very short length of shifts (typically under a minute) contribute to the difficulty of isolating the impact of players on the results of the game.\nNonetheless, steps have been taken over the past two decades to improve the state of hockey analytics. At the beginning of the sport, the goals and assists by players were recorded by officials and coaches and for a long time that was the only data collected on the game. The earliest attempt at an advanced statistic in hockey was plus-minus. Plus-minus is calculated for each player by adding the goals scored for the team while the player is on the ice and subtracting the goals scored against the team while the player is on the ice. Plus-minus runs into the same issue as many statistics in hockey where it is difficult to separate individual players’ contributions and strengths from the overall strength of their team and their line because players are usually on the ice with the same people at the same time.\nAround the mid 2000s, people began to look beyond goals scored and allowed as the only data points that can be used to evaluate players. Some of the other factors considered are shots taken, penalty differential, quality of opponents, and the usage of players based on the zone location of the puck when they are deployed. One of the early popular evaluation metrics was Corsi. Corsi is calculated by taking the sum of shots on goal, missed shots, and blocked shots. In following years Corsi was modified to different iterations that account for some of the problems with the original metric such as Corsi relative (compares individual player to the rest of their team), Corsi close (only considers situations where the game is within one goal), and score adjusted Corsi (places shot attempts in the context of expectations based on the current score.) The major problem that remained after these adjustments is that the metric still doesn’t account for the quality of the shots taken.\nMore recent research into hockey analytics metrics have focused on shot quality and expected goals. Shot quality was first brought up in 2004 and was found by cataloguing shot types and whether or not they were rebounds. Goal probability curves were fit to the shot types. The expected goal metric comes from summing the total goal probabilities. More modern research has focused on improving the expected goals model. Brian Macdonald (2012) created an expected goals metric using least squares regression and ridge regression models. In 2015, Sprigings and Toumi made a model that incorporated shot type, shot angle, distance from net, whether the shooter was shooting off their strong side, and a value of the player’s shooting talent.\nThese new metrics have advanced analytics applications to the sport of hockey and have shown predictive power when looking at team and player success. However, often times a very important aspect of play in hockey is overlooked when applying these metrics: power plays and penalty kills.\nPower plays and penalty kills (together called special teams opportunities) are scenarios in which one team has an extra player on the ice compared to the other after a penalty. These scenarios vary a lot game to game which is why they are excluded from metrics, but they also are very important parts of the game outcome. About one third of goals scored in hockey games come from power plays (33.5% of goals in the olympics; 29% of goals in Finland’s hockey league). In this project I would like to explore the impact that power plays and penalty kills have on the outcome of games and seek answers to the question of how a coach should best approach special teams situations. I will look to address the following questions:\n\nHow does a team’s rate of power play goal success relate to their regular season success?\nHow does a team’s rate of penalty kill goals allowed relate to their regular season success?\nAre these relationships the same for post season success?\nWhat kind of personnel can be used in special teams situations?\nHow do player success metrics outside of special teams opportunities relate to their success in special teams opportunities?\nHow does incorporating data from special teams opportunities into overall season metrics affect the predictivity of the metrics?\nWhich player attributes are most important for power play success?\nWhich player attributes are most important for penalty kill success?\nWhat effect does a successful power play have on winning the game?\nWhat effect does a successful penalty kill have on winning the game?"
  },
  {
    "objectID": "exploratory_data_analysis.html",
    "href": "exploratory_data_analysis.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nregular_data &lt;- read.csv(\"../data/01-modified-data/skaters_clean.csv\")\npp_data &lt;- read.csv(\"../data/01-modified-data/skaters_pp.csv\")\npk_data &lt;- read.csv(\"../data/01-modified-data/skaters_pk.csv\")\n\n\nsummary_data_regular &lt;- regular_data %&gt;%\n  subset(select=c(expectedGoals,corsi,shotAttempts,points,goals,faceoffsWon,takeways,giveaways,hits)) %&gt;%\n  summary()\nsummary_data_regular\n\n expectedGoals        corsi         shotAttempts       points     \n Min.   :0.3200   Min.   :0.3800   Min.   : 45.0   Min.   : 2.00  \n 1st Qu.:0.4600   1st Qu.:0.4700   1st Qu.:128.0   1st Qu.:14.00  \n Median :0.5000   Median :0.5000   Median :173.0   Median :20.00  \n Mean   :0.4981   Mean   :0.4983   Mean   :181.3   Mean   :22.38  \n 3rd Qu.:0.5300   3rd Qu.:0.5300   3rd Qu.:219.0   3rd Qu.:29.00  \n Max.   :0.6400   Max.   :0.6500   Max.   :443.0   Max.   :65.00  \n     goals         faceoffsWon        takeways       giveaways    \n Min.   : 0.000   Min.   :  0.00   Min.   : 1.00   Min.   : 2.00  \n 1st Qu.: 4.000   1st Qu.:  0.00   1st Qu.:15.00   1st Qu.:16.00  \n Median : 7.000   Median :  4.00   Median :22.00   Median :25.00  \n Mean   : 8.376   Mean   : 89.22   Mean   :24.51   Mean   :26.92  \n 3rd Qu.:12.000   3rd Qu.:113.50   3rd Qu.:32.00   3rd Qu.:35.00  \n Max.   :34.000   Max.   :739.00   Max.   :77.00   Max.   :89.00  \n      hits      \n Min.   :  5.0  \n 1st Qu.: 43.0  \n Median : 69.0  \n Mean   : 81.2  \n 3rd Qu.:108.0  \n Max.   :302.0  \n\n\n\nsummary(pp_data$icetime)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      1     414    4313    5874   10288   19733 \n\n#remove bottom 25% of players by ice time\npp_data &lt;- pp_data %&gt;%\n  filter(icetime &gt; 414)\n\n#summarize all metrics\nsummary_data_pp &lt;- pp_data %&gt;%\n  subset(select=c(expectedGoals,corsi,shotAttempts,points,goals,faceoffsWon,takeways,giveaways,hits)) %&gt;%\n  summary()\nsummary_data_pp\n\n expectedGoals        corsi         shotAttempts       points      \n Min.   :0.2400   Min.   :0.6000   Min.   :  0.0   Min.   : 0.000  \n 1st Qu.:0.8200   1st Qu.:0.8300   1st Qu.:  9.0   1st Qu.: 1.000  \n Median :0.8700   Median :0.8600   Median : 34.0   Median : 7.000  \n Mean   :0.8502   Mean   :0.8509   Mean   : 43.8   Mean   : 9.434  \n 3rd Qu.:0.9000   3rd Qu.:0.8800   3rd Qu.: 67.0   3rd Qu.:14.000  \n Max.   :1.0000   Max.   :1.0000   Max.   :208.0   Max.   :64.000  \n     goals         faceoffsWon       takeways        giveaways     \n Min.   : 0.000   Min.   :  0.0   Min.   : 0.000   Min.   : 0.000  \n 1st Qu.: 0.000   1st Qu.:  0.0   1st Qu.: 0.000   1st Qu.: 1.000  \n Median : 2.000   Median :  1.0   Median : 1.000   Median : 3.000  \n Mean   : 3.226   Mean   : 15.4   Mean   : 1.262   Mean   : 3.941  \n 3rd Qu.: 5.000   3rd Qu.: 10.0   3rd Qu.: 2.000   3rd Qu.: 6.000  \n Max.   :30.000   Max.   :195.0   Max.   :10.000   Max.   :27.000  \n      hits       \n Min.   : 0.000  \n 1st Qu.: 0.000  \n Median : 1.000  \n Mean   : 1.879  \n 3rd Qu.: 3.000  \n Max.   :15.000  \n\n\n\nsummary(pk_data$icetime)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      0     555    4323    4744    8158   17003 \n\n#remove bottom 25% of players by ice time\npk_data &lt;- pk_data %&gt;%\n  filter(icetime &gt; 555)\n\n#summarize all metrics\nsummary_data_pk &lt;- pk_data %&gt;%\n  subset(select=c(expectedGoals,corsi,shotAttempts,points,goals,faceoffsWon,takeways,giveaways,hits)) %&gt;%\n  summary()\nsummary_data_pk\n\n expectedGoals        corsi         shotAttempts        points      \n Min.   :0.0000   Min.   :0.0000   Min.   : 0.000   Min.   :0.0000  \n 1st Qu.:0.0900   1st Qu.:0.1100   1st Qu.: 3.000   1st Qu.:0.0000  \n Median :0.1300   Median :0.1300   Median : 5.000   Median :1.0000  \n Mean   :0.1424   Mean   :0.1434   Mean   : 6.757   Mean   :0.9588  \n 3rd Qu.:0.1700   3rd Qu.:0.1700   3rd Qu.:10.000   3rd Qu.:1.0000  \n Max.   :0.7300   Max.   :0.6200   Max.   :47.000   Max.   :7.0000  \n     goals         faceoffsWon        takeways        giveaways     \n Min.   :0.0000   Min.   :  0.00   Min.   : 0.000   Min.   : 0.000  \n 1st Qu.:0.0000   1st Qu.:  0.00   1st Qu.: 1.000   1st Qu.: 0.000  \n Median :0.0000   Median :  0.00   Median : 3.000   Median : 1.000  \n Mean   :0.4469   Mean   : 12.95   Mean   : 3.848   Mean   : 2.098  \n 3rd Qu.:1.0000   3rd Qu.: 11.00   3rd Qu.: 6.000   3rd Qu.: 3.000  \n Max.   :4.0000   Max.   :152.00   Max.   :21.000   Max.   :16.000  \n      hits       \n Min.   : 0.000  \n 1st Qu.: 1.000  \n Median : 3.000  \n Mean   : 4.605  \n 3rd Qu.: 6.000  \n Max.   :33.000"
  },
  {
    "objectID": "exploratory_data_analysis.html#summary-statistics",
    "href": "exploratory_data_analysis.html#summary-statistics",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nregular_data &lt;- read.csv(\"../data/01-modified-data/skaters_clean.csv\")\npp_data &lt;- read.csv(\"../data/01-modified-data/skaters_pp.csv\")\npk_data &lt;- read.csv(\"../data/01-modified-data/skaters_pk.csv\")\n\n\nsummary_data_regular &lt;- regular_data %&gt;%\n  subset(select=c(expectedGoals,corsi,shotAttempts,points,goals,faceoffsWon,takeways,giveaways,hits)) %&gt;%\n  summary()\nsummary_data_regular\n\n expectedGoals        corsi         shotAttempts       points     \n Min.   :0.3200   Min.   :0.3800   Min.   : 45.0   Min.   : 2.00  \n 1st Qu.:0.4600   1st Qu.:0.4700   1st Qu.:128.0   1st Qu.:14.00  \n Median :0.5000   Median :0.5000   Median :173.0   Median :20.00  \n Mean   :0.4981   Mean   :0.4983   Mean   :181.3   Mean   :22.38  \n 3rd Qu.:0.5300   3rd Qu.:0.5300   3rd Qu.:219.0   3rd Qu.:29.00  \n Max.   :0.6400   Max.   :0.6500   Max.   :443.0   Max.   :65.00  \n     goals         faceoffsWon        takeways       giveaways    \n Min.   : 0.000   Min.   :  0.00   Min.   : 1.00   Min.   : 2.00  \n 1st Qu.: 4.000   1st Qu.:  0.00   1st Qu.:15.00   1st Qu.:16.00  \n Median : 7.000   Median :  4.00   Median :22.00   Median :25.00  \n Mean   : 8.376   Mean   : 89.22   Mean   :24.51   Mean   :26.92  \n 3rd Qu.:12.000   3rd Qu.:113.50   3rd Qu.:32.00   3rd Qu.:35.00  \n Max.   :34.000   Max.   :739.00   Max.   :77.00   Max.   :89.00  \n      hits      \n Min.   :  5.0  \n 1st Qu.: 43.0  \n Median : 69.0  \n Mean   : 81.2  \n 3rd Qu.:108.0  \n Max.   :302.0  \n\n\n\nsummary(pp_data$icetime)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      1     414    4313    5874   10288   19733 \n\n#remove bottom 25% of players by ice time\npp_data &lt;- pp_data %&gt;%\n  filter(icetime &gt; 414)\n\n#summarize all metrics\nsummary_data_pp &lt;- pp_data %&gt;%\n  subset(select=c(expectedGoals,corsi,shotAttempts,points,goals,faceoffsWon,takeways,giveaways,hits)) %&gt;%\n  summary()\nsummary_data_pp\n\n expectedGoals        corsi         shotAttempts       points      \n Min.   :0.2400   Min.   :0.6000   Min.   :  0.0   Min.   : 0.000  \n 1st Qu.:0.8200   1st Qu.:0.8300   1st Qu.:  9.0   1st Qu.: 1.000  \n Median :0.8700   Median :0.8600   Median : 34.0   Median : 7.000  \n Mean   :0.8502   Mean   :0.8509   Mean   : 43.8   Mean   : 9.434  \n 3rd Qu.:0.9000   3rd Qu.:0.8800   3rd Qu.: 67.0   3rd Qu.:14.000  \n Max.   :1.0000   Max.   :1.0000   Max.   :208.0   Max.   :64.000  \n     goals         faceoffsWon       takeways        giveaways     \n Min.   : 0.000   Min.   :  0.0   Min.   : 0.000   Min.   : 0.000  \n 1st Qu.: 0.000   1st Qu.:  0.0   1st Qu.: 0.000   1st Qu.: 1.000  \n Median : 2.000   Median :  1.0   Median : 1.000   Median : 3.000  \n Mean   : 3.226   Mean   : 15.4   Mean   : 1.262   Mean   : 3.941  \n 3rd Qu.: 5.000   3rd Qu.: 10.0   3rd Qu.: 2.000   3rd Qu.: 6.000  \n Max.   :30.000   Max.   :195.0   Max.   :10.000   Max.   :27.000  \n      hits       \n Min.   : 0.000  \n 1st Qu.: 0.000  \n Median : 1.000  \n Mean   : 1.879  \n 3rd Qu.: 3.000  \n Max.   :15.000  \n\n\n\nsummary(pk_data$icetime)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      0     555    4323    4744    8158   17003 \n\n#remove bottom 25% of players by ice time\npk_data &lt;- pk_data %&gt;%\n  filter(icetime &gt; 555)\n\n#summarize all metrics\nsummary_data_pk &lt;- pk_data %&gt;%\n  subset(select=c(expectedGoals,corsi,shotAttempts,points,goals,faceoffsWon,takeways,giveaways,hits)) %&gt;%\n  summary()\nsummary_data_pk\n\n expectedGoals        corsi         shotAttempts        points      \n Min.   :0.0000   Min.   :0.0000   Min.   : 0.000   Min.   :0.0000  \n 1st Qu.:0.0900   1st Qu.:0.1100   1st Qu.: 3.000   1st Qu.:0.0000  \n Median :0.1300   Median :0.1300   Median : 5.000   Median :1.0000  \n Mean   :0.1424   Mean   :0.1434   Mean   : 6.757   Mean   :0.9588  \n 3rd Qu.:0.1700   3rd Qu.:0.1700   3rd Qu.:10.000   3rd Qu.:1.0000  \n Max.   :0.7300   Max.   :0.6200   Max.   :47.000   Max.   :7.0000  \n     goals         faceoffsWon        takeways        giveaways     \n Min.   :0.0000   Min.   :  0.00   Min.   : 0.000   Min.   : 0.000  \n 1st Qu.:0.0000   1st Qu.:  0.00   1st Qu.: 1.000   1st Qu.: 0.000  \n Median :0.0000   Median :  0.00   Median : 3.000   Median : 1.000  \n Mean   :0.4469   Mean   : 12.95   Mean   : 3.848   Mean   : 2.098  \n 3rd Qu.:1.0000   3rd Qu.: 11.00   3rd Qu.: 6.000   3rd Qu.: 3.000  \n Max.   :4.0000   Max.   :152.00   Max.   :21.000   Max.   :16.000  \n      hits       \n Min.   : 0.000  \n 1st Qu.: 1.000  \n Median : 3.000  \n Mean   : 4.605  \n 3rd Qu.: 6.000  \n Max.   :33.000"
  },
  {
    "objectID": "exploratory_data_analysis.html#normalization-of-data",
    "href": "exploratory_data_analysis.html#normalization-of-data",
    "title": "Exploratory Data Analysis",
    "section": "Normalization of Data",
    "text": "Normalization of Data\n\nnorm_regular_data&lt;-mutate(regular_data, expectedGoals=expectedGoals/mean(expectedGoals), corsi=corsi/mean(corsi), shotAttempts=shotAttempts/mean(shotAttempts), points=points/mean(points), goals=goals/mean(goals), faceoffsWon=faceoffsWon/mean(faceoffsWon),takeways=takeways/mean(takeways),giveaways=giveaways/mean(giveaways),hits=hits/mean(hits))\n\nnorm_pp_data&lt;-mutate(pp_data, expectedGoals=expectedGoals/mean(expectedGoals), corsi=corsi/mean(corsi), shotAttempts=shotAttempts/mean(shotAttempts), points=points/mean(points), goals=goals/mean(goals), faceoffsWon=faceoffsWon/mean(faceoffsWon),takeways=takeways/mean(takeways),giveaways=giveaways/mean(giveaways),hits=hits/mean(hits))\n \nnorm_pk_data&lt;-mutate(pk_data, expectedGoals=expectedGoals/mean(expectedGoals), corsi=corsi/mean(corsi), shotAttempts=shotAttempts/mean(shotAttempts), points=points/mean(points), goals=goals/mean(goals), faceoffsWon=faceoffsWon/mean(faceoffsWon),takeways=takeways/mean(takeways),giveaways=giveaways/mean(giveaways),hits=hits/mean(hits))"
  },
  {
    "objectID": "exploratory_data_analysis.html#check-for-outliers",
    "href": "exploratory_data_analysis.html#check-for-outliers",
    "title": "Exploratory Data Analysis",
    "section": "Check for Outliers",
    "text": "Check for Outliers\n\nnorm_regular_data&lt;-norm_regular_data %&gt;%\n  subset(select=c(ID, name, team, expectedGoals,corsi,shotAttempts,points,goals,faceoffsWon,takeways,giveaways,hits))\nsapply(norm_regular_data, function(x) sum(x &gt; 3)) #faceoffs won has a lot of outliers\n\n           ID          name          team expectedGoals         corsi \n          615           615           615             0             0 \n shotAttempts        points         goals   faceoffsWon      takeways \n            0             0             9            95             2 \n    giveaways          hits \n            3             8 \n\n\n\nnorm_pp_data&lt;-norm_pp_data %&gt;%\n  subset(select=c(ID, name, team, expectedGoals,corsi,shotAttempts,points,goals,faceoffsWon,takeways,giveaways,hits))\nsapply(norm_pp_data, function(x) sum(x &gt; 3))  #most metrics have larger number of outliers for powerplays than regular play\n\n           ID          name          team expectedGoals         corsi \n          461           461           461             0             0 \n shotAttempts        points         goals   faceoffsWon      takeways \n           17            19            38            54            47 \n    giveaways          hits \n           23            28 \n\n\n\nnorm_pk_data&lt;-norm_pk_data %&gt;%\n  subset(select=c(ID, name, team, expectedGoals,corsi,shotAttempts,points,goals,faceoffsWon,takeways,giveaways,hits))\nsapply(norm_pk_data, function(x) sum(x &gt; 3))  #most metrics have larger number of outliers for penalty kills than regular play\n\n           ID          name          team expectedGoals         corsi \n          461           461           461             7             3 \n shotAttempts        points         goals   faceoffsWon      takeways \n            9            48            50            65            16 \n    giveaways          hits \n           24            36"
  },
  {
    "objectID": "exploratory_data_analysis.html#compare-regular-play-and-powerplays",
    "href": "exploratory_data_analysis.html#compare-regular-play-and-powerplays",
    "title": "Exploratory Data Analysis",
    "section": "Compare Regular Play and Powerplays",
    "text": "Compare Regular Play and Powerplays\n\n#join pp data with regular data\ncolnames(norm_pp_data) &lt;- c(\"ID\",\"name\",\"team\",\"expectedGoals_pp\",\"corsi_pp\",\"shotAttempts_pp\",\"points_pp\",\"goals_pp\",\"faceoffsWon_pp\",\"takeaways_pp\",\"giveaways_pp\",\"hits_pp\")\ndf.1 &lt;- left_join(norm_pp_data, norm_regular_data, by=\"ID\")\n\n\n#compare metric values between regular play and powerplays for all players\n\nlibrary(ggplot2)\n\nggplot(data=df.1) + geom_point(aes(x=expectedGoals, y=expectedGoals_pp))\n\n\n\nggplot(data=df.1) + geom_point(aes(x=corsi, y=corsi_pp))\n\n\n\nggplot(data=df.1) + geom_point(aes(x=shotAttempts, y= shotAttempts_pp))\n\n\n\nggplot(data=df.1) + geom_point(aes(x=points, y=points_pp))\n\n\n\nggplot(data=df.1) + geom_point(aes(x=goals, y=goals_pp))\n\n\n\nggplot(data=df.1) + geom_point(aes(x=faceoffsWon, y=faceoffsWon_pp))\n\n\n\nggplot(data=df.1) + geom_point(aes(x=takeways, y=takeaways_pp))\n\n\n\nggplot(data=df.1) + geom_point(aes(x=giveaways, y=giveaways_pp))\n\n\n\nggplot(data=df.1) + geom_point(aes(x=hits, y=hits_pp))"
  },
  {
    "objectID": "exploratory_data_analysis.html#compare-regular-play-and-penalty-kills",
    "href": "exploratory_data_analysis.html#compare-regular-play-and-penalty-kills",
    "title": "Exploratory Data Analysis",
    "section": "Compare Regular Play and Penalty Kills",
    "text": "Compare Regular Play and Penalty Kills\n\n#join pk data with regular data\ncolnames(norm_pk_data) &lt;- c(\"ID\",\"name\",\"team\",\"expectedGoals_pk\",\"corsi_pk\",\"shotAttempts_pk\",\"points_pk\",\"goals_pk\",\"faceoffsWon_pk\",\"takeaways_pk\",\"giveaways_pk\",\"hits_pk\")\ndf.2 &lt;- left_join(norm_pk_data, norm_regular_data, by=\"ID\")\n\n\n#compare metric values between regular play and penalty kills for all players\n\nlibrary(ggplot2)\n\nggplot(data=df.2) + geom_point(aes(x=expectedGoals, y=expectedGoals_pk))\n\n\n\nggplot(data=df.2) + geom_point(aes(x=corsi, y=corsi_pk))\n\n\n\nggplot(data=df.2) + geom_point(aes(x=shotAttempts, y= shotAttempts_pk))\n\n\n\nggplot(data=df.2) + geom_point(aes(x=points, y=points_pk))\n\n\n\nggplot(data=df.2) + geom_point(aes(x=goals, y=goals_pk))\n\n\n\nggplot(data=df.2) + geom_point(aes(x=faceoffsWon, y=faceoffsWon_pk))\n\n\n\nggplot(data=df.2) + geom_point(aes(x=takeways, y=takeaways_pk))\n\n\n\nggplot(data=df.2) + geom_point(aes(x=giveaways, y=giveaways_pk))\n\n\n\nggplot(data=df.2) + geom_point(aes(x=hits, y=hits_pk))"
  },
  {
    "objectID": "exploratory_data_analysis.html#compare-powerplay-and-penalty-kill-success-to-team-record",
    "href": "exploratory_data_analysis.html#compare-powerplay-and-penalty-kill-success-to-team-record",
    "title": "Exploratory Data Analysis",
    "section": "Compare Powerplay and Penalty Kill Success to Team Record",
    "text": "Compare Powerplay and Penalty Kill Success to Team Record\n\nFind powerplay and penalty kill success rates for all teams in 2022 season\nFind data with points record for all teams in 2022 season\nAdd variable for each team that indicates whether they made the playoffs\nFind correlation between pp and pk success and team record\n\n\n#load team data and team record data\nteam_data &lt;- read.csv(\"../data/01-modified-data/team_record_clean.csv\")\n\n\n#compare powerplay success to overall season success\nggplot(data=team_data) + geom_point(aes(x=ppPercent, y=points))\n\n\n\nggplot(data=team_data) + geom_point(aes(x=ppGoals, y=points))\n\n\n\n#compare penalty kill success to overall season success\nggplot(data=team_data) + geom_point(aes(x=pkPercent, y=points))\n\n\n\nggplot(data=team_data) + geom_point(aes(x=pkGoalsAgainst, y=points))"
  },
  {
    "objectID": "naive_bayes.html",
    "href": "naive_bayes.html",
    "title": "Introduction to Naive Bayes",
    "section": "",
    "text": "The Naive Bayes classification machine learning model uses Bayes Theorem to classify data points into one of several labels. When multiple features are used, the model calculates the probability of each label being the true label based on the conditional probability from the state of the feature. This is done for each of the features present, and the model assigns the label with the highest probability.\nThe Naive Bayes model operates under the assumption that the effects of the different features are independent of one another.\nGiven my prior hypothesis that certain team evaluation metrics during powerplays may be more predictive of regular season team success, I am looking to use Naive Bayes classification to determine which features are most predictive and then determine if these features during power play situations can be more accurately used to train a ML model to predict playoff berths then features from regular 5 on 5 game play.\n\nPrepare Data for Naive Bayes\nI prepared the team powerplay data for Naive Bayes classification in an R file linked here. Team data from the 2018-2019, 2020-2021, 2021-2022, and 2022-2023 seasons were combined and the desired features were subsetted from the greater dataset. An additional variable was added for whether or not the team made the playoffs that season and all numerical metrics were normalized. Finally, all non-numerical variables except the playoff variable were removed.\n\nimport numpy as np\nimport pandas as pd\n\n\ndf=pd.read_csv(\"../data/01-modified-data/team_pp_data_clean.csv\")\nprint(df.shape)\n\nlabel = df['playoff'].copy()\ndf=df.drop(columns=['Unnamed: 0','playoff'])\nfeatures=df.columns\nfeature_matrix = df[features].copy()\n\n\n(126, 10)\n\n\n\nfrom sklearn.model_selection import train_test_split\n\nX = feature_matrix\ny= label\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size=0.2,\n    random_state=100\n)\n\nAfter loading the pre-prepared dataset, I used sklearn to split the dataset into a training set and a test set with an 80-20 split. It is important to keep a portion of the data aside to test the accuracy of the model on.\n\n\nFeature Selection\nFeature selection is a process by which only the most predictive and least correlated features are used to train a model in order to maximize accuracy.\nTo find the subset of features that result in the highest accuracy score, I will find every possible combination of the seven features and calculate a metric to evaluate their correlation coefficients.\n\nfrom itertools import chain, combinations\n\n#list all possible feature subsets\nfeature_subsets = list(features)\nfeature_subset=chain.from_iterable(combinations(feature_subsets,r) for r in range(len(feature_subsets)+1))\nfeature_subset=list(feature_subset)\n\n\n\n\nX_train_df=pd.DataFrame(X_train, columns=features)\nX_test_df=pd.DataFrame(X_test, columns=features)\n\nfrom scipy.stats import spearmanr\nimport itertools\n\n#calculate spearman correlation coefficients for each subset\ndef mean_xx_corr(x_df):\n    df_colnames=x_df.columns\n    xx_corrs=[]\n\n    df_colname_pairs=itertools.combinations(df_colnames, 2)\n    for colname1, colname2 in df_colname_pairs:\n        col1=x_df[colname1]\n        col2=x_df[colname2]\n        xx_pair_corr=spearmanr(col1, col2).stastic\n        xx_corrs.append(xx_pair_corr)\n    \n    return np.mean(xx_corrs)\n\n\ndef compute_mean_xy_corr(x_df, y_vec):\n    df_colnames=x_df.columns\n    xy_corrs=[]\n    for colname in df_colnames:\n        x_col = x_df[colname]\n        xy_pair_corr = spearmanr(x_col, y_vec)\n        xy_corrs.append(xy_pair_corr)\n\n    return np.mean(xy_corrs)\n\n     xGoalsPercentage  corsiPercentage  shotAttemptsFor  xGoalsFor  \\\n70           1.006796         1.022595         0.850992   0.722261   \n72           0.972079         0.976114         0.640017   0.695782   \n68           1.053086         1.022595         0.815534   0.802483   \n41           1.018369         0.999354         1.065513   1.289058   \n82           0.983652         0.999354         0.842127   0.776004   \n..                ...              ...              ...        ...   \n87           1.064658         1.034216         0.808442   0.998581   \n103          0.925790         0.952873         1.063740   1.069103   \n67           0.972079         0.976114         0.682566   0.731174   \n24           1.041514         1.022595         1.196707   1.294826   \n8            0.937362         0.976114         1.147066   1.024535   \n\n     faceOffsWonFor  takeawaysFor  giveawaysFor   hitsFor  \n70         0.790867      0.506702      0.597073  0.619545  \n72         0.617865      0.731903      0.807805  0.774431  \n68         0.677180      0.844504      0.720000  0.580824  \n41         1.176415      0.844504      0.878049  0.580824  \n82         0.899612      0.675603      0.509268  1.084204  \n..              ...           ...           ...       ...  \n87         0.815582      0.619303      0.842927  0.193608  \n103        1.122043      0.957105      1.440000  0.890596  \n67         0.687066      0.788204      0.649756  0.929318  \n24         1.359303      0.394102      0.772683  1.200369  \n8          0.988584      0.957105      1.159024  0.968039  \n\n[100 rows x 8 columns]\n\n\nInvalidIndexError: (0, slice(4, 5, None))\n\n\n\n\nNaive Bayes\nThe first NB model I used incorporated the data from all seven features. The GaussianNB model from the sklearn package was trained on the previously partitioned train data.\n\nfrom sklearn.naive_bayes import GaussianNB\n\nmodel=GaussianNB()\n\nmodel.fit(X_train, y_train)\n\nfrom sklearn.metrics import accuracy_score \nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\n\ny_pred=model.predict(X_test)\naccuracy=accuracy_score(y_pred, y_test)\nf1 = f1_score(y_pred, y_test, average=\"weighted\")\n\nprint(accuracy)\nprint(f1)\n\ncm=confusion_matrix(y_test, y_pred)\ndisp=ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Non-playoff team\",\"Playoff team\"])\ndisp.plot()\n\n\n0.8076923076923077\n0.809839283523494\n\n\n&lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7fb3821aa800&gt;\n\n\n\n\n\nAfter training on the training data, I had the model predict whether the records in the test data were playoff teams based on their features. The accuracy and f1 scores were both around 80% which is pretty high and the confusion matrix shows that the model was equally proficient at predicting teams to make the playoffs as they are at predicting teams to miss the playoffs.\nFurther, I would like to test other subsets of features to find a more accurate model. And then carry out the same process to evaluate models based on regular 5 on 5 play and compare the accuracy of these two models.\nFor now, it is clear to see that player evaluation metrics during power plays can be used to predict regular season success."
  }
]