[
  {
    "objectID": "Clustering.html",
    "href": "Clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "In this part of the project I will be clustering a data point for each NHL team from the 2022 season based on the 7 features in the dataset that I have been previously working with. With the clustering analysis, I aim to elucidate the number of categories of teams based on quality of play in the season. There are traditionally thought to be 3 main types of teams: playoff teams, teams that are in contention and miss the playoffs, and teams that are out of playoff contention most of the season. Finding the ideal number of clusters and seeing if these clusters align with the categories of teams will give insight as to how accurate this assertion is."
  },
  {
    "objectID": "Clustering.html#theory",
    "href": "Clustering.html#theory",
    "title": "Clustering",
    "section": "Theory",
    "text": "Theory\nClustering is an unsupervised machine learning model, meaning that the data that the model is trained on does not have labels. The model is therefore searching for innate similarities and differences between data points in the data set to group them by. There is also no set target to achieve with the model. A major objective, and the one I have in this tab of the portfolio, is to determine the number of clusters in a dataset.\n\nK-Means\nThere are many different methods of clustering based on features but I will be looking at 3 specifically in this analysis. The first of the three is K-Means clustering. In K-Means clustering, a set number (based on a hyperparameter) of centroids are set within the data and each data point is assigned to the centroid with the closest mean. The centroid of the cluster is considered the most representative datapoint of the cluster.\nIn K-means clustering regular Euclidean distance is not used, but rather squared Euclidean distances are minimized in the cluster assigning process. In the first round of clustering, random centroids (number is based on a hyperparameter) are assigned within the data and all observations are assigned to a centroid. With the newly assigned clusters, new centroids are calculated as the average value of the observations in the cluster and points are reassigned based on distance to the new centroids. This process is repeated until calculating new centroids does not change the cluster assignment.\n\n\nDBSCAN\nThe second is DBSCAN. In DBSCAN clustering, density of points is used to make clusters. Groups of points that are located very close together are grouped in a cluster together and are typically separated by areas of low point density. DBSCAN clustering also marks outliers which are points that are not close enough to any other points to have a cluster.\nDBSCAN clustering is useful for data sets with unusual patterns because it can create clusters that are not separated by simple geographic shapes.\n\n\nHierarchical Clustering\nThe third is hierarchical clustering. In agglomerative hierarchical clustering, the model moves through a series of steps in which the two most similar points are clustered together and then the most similar clusters are merged together until the number of clusters is sufficiently small. There is also division hierarchical clustering in which all of the data points start in one cluster and are separated down the hierarchy.\nThe optimal number of clusters can be determined using hyperparameter tuning. A major difference between hierarchical clustering and other methods is that hierarchical clustering does not assume an assigned number of clusters."
  },
  {
    "objectID": "Clustering.html#method",
    "href": "Clustering.html#method",
    "title": "Clustering",
    "section": "Method",
    "text": "Method\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport sklearn\nimport sklearn.cluster\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\nfrom scipy.spatial.distance import cdist\nfrom sklearn.manifold import TSNE\n\n\nTo start the process of finding the best clusters in the data, I imported the team power point data, removed the label column, and created a feature_matrix. I decided to use the variables that were created on the Dimensionality Reduction tab using TSNE as the observations for clustering so that the clusters will be easy to visualize in two dimensions.\n\n\nCode\ndf=pd.read_csv(\"../data/01-modified-data/team_pp_data_point_bracket_clean.csv\")\n\nlabel = df['points_cat'].copy()\ndf=df.drop(columns=['Unnamed: 0','points_cat'])\ndf_embedded=TSNE(n_components=2,learning_rate='auto',init='random',perplexity=15).fit_transform(df)\n\nfeatures=df.columns\nfeature_matrix = df[features].copy()\n\nembedd_df=pd.DataFrame(df_embedded)\n\nlabeled_features=pd.concat((embedd_df,label), axis=1, join='inner')\nlabeled_features.columns=['feature_1','feature_2','label']\nprint(labeled_features)\n\n\n\n    feature_1  feature_2     label\n0   -3.854289   4.747375  mid_tier\n1   -6.185002   5.292325   over100\n2   -4.212695   8.358073   under80\n3   -3.012514  10.011864   under80\n4   -5.152484   3.917608  mid_tier\n5   -3.310389   5.184929  mid_tier\n6   -5.652327   7.284412   over100\n7   -6.331969   6.676617  mid_tier\n8   -3.931947   8.989484  mid_tier\n9   -4.530917  10.714497   over100\n10  -4.319849  10.348486  mid_tier\n11  -1.310789  10.292958   under80\n12  -6.330052   6.191101  mid_tier\n13  -4.581445   9.385482  mid_tier\n14  -5.106497   9.733096   over100\n15  -6.945577   7.511106   over100\n16  -2.449926   4.417172   over100\n17  -6.483069   8.158342   over100\n18  -3.496441   8.232561   under80\n19  -5.681571   8.507524   over100\n20  -2.056978   8.668019   over100\n21  -1.902181   9.896543   under80\n22  -2.125180   6.755563   under80\n23  -3.321953  11.137379   under80\n24  -0.668504   9.190001   over100\n25  -4.917902   4.069370  mid_tier\n26  -3.683078   5.253610  mid_tier\n27  -2.926841   8.124903   over100\n28  -1.660859   8.370879   over100\n29  -2.511519   9.093883  mid_tier\n30  -6.432148   9.269286  mid_tier\n31  -3.126646   6.473274  mid_tier\n\n\n\nK-Means\nUsing the feature matrix generated above with TSNE, I then used some distance caluclations to determine the best number of clusters to use with this data in K-means clustering. I iterated over the range 1 to 11 for the value of number of clusters in a K-Means cluster model and calculated the given models distortions and inertias.\n\n\nCode\nK=range(1,11)\ndistortions=[]\ninertias=[]\n\nfor k in K:\n    model=sklearn.cluster.KMeans(n_clusters=k).fit(df_embedded)\n    model.fit(df_embedded)\n\n    inertias.append(model.inertia_)\n    distortions.append(sum(np.min(cdist(df_embedded, model.cluster_centers_, 'euclidean'), axis=1)) / df_embedded.shape[0])\n\n\n/Users/gracehyland/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/gracehyland/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/gracehyland/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/gracehyland/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/gracehyland/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/gracehyland/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/gracehyland/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/gracehyland/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/gracehyland/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/gracehyland/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/gracehyland/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/gracehyland/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/gracehyland/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/gracehyland/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/gracehyland/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/gracehyland/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/gracehyland/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/gracehyland/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/gracehyland/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/gracehyland/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\nThese values for distortions and inertias can then be used to create elbow plots to determine the ideal number of clusters. First, I had to make a data frame that included the number of clusters, the distortion values, and the inertia values.\nI then graphed both distortion and inertia against the number of clusters used to creat an elbow plot.\n\n\nCode\nK=[1,2,3,4,5,6,7,8,9,10]\n\n\ndf2=pd.DataFrame(\n    {'K': K,\n     'distortions': distortions,\n     'inertias': inertias\n     }\n)\nplt.plot(df2['distortions'])\nplt.xlabel(xlabel=\"Number of clusters\")\nplt.ylabel(ylabel=\"Distortion\")\nplt.show()\n\nplt.plot(df2['inertias'])\nplt.xlabel(xlabel=\"Number of clusters\")\nplt.ylabel(ylabel=\"Inertia\")\nplt.show()\n\n\n\n\n\n\n\n\nBoth the inertia plot and the distortion plot show a leveling off after the value of three clusters without much improvement in the similarity of the clusters beyond three. Given this, the ideal number of clusters for this data when using K-Means clustering is three.\n\n\nHierarchical clustering\nUsing the same feature_matrix as above, I calculated the silhouette score for each value of the hyperparameter nmax in agglomerative hierachical clustering. This hyperparameter dictates how many steps the model can move up the hierarchy when merging clusters together to create more clusters. I evaluated the model for the range 0 to 10 and added the silhouette score for each value to a list.\n\n\nCode\nnmax=10\nsil_max=0\ndf4=np.ascontiguousarray(df_embedded)\n\nparams=[]\nsil_scores=[]\nfor k in range(2,nmax+1):\n    model=sklearn.cluster.AgglomerativeClustering(n_clusters=k).fit(df4)\n    labels=model.labels_\n\n    sil_scores.append(sklearn.metrics.silhouette_score(df4, labels))\n    params.append(k)\n\n    if(sil_scores[-1]&gt;sil_max):\n        opt_param=k\n        sil_max=sil_scores[-1]\n        opt_labels=labels\n\nfig, ax = plt.subplots()\nax.plot(params, sil_scores, \"-o\")\nax.set(xlabel='Hyper-parameter', ylabel='Silhouette')\nplt.show()\n\n\nprint(\"OPTIMAL PARAMETER=\", opt_param)\n\n\n\n\n\nOPTIMAL PARAMETER= 3\n\n\nUsing the list of slihouette scores, I made a plot of silhouette score against nmax values and found that the silhouette score maxes out when nmax is equal to three. Therefore the optimal hyperparameter is nmax=3."
  },
  {
    "objectID": "Clustering.html#results",
    "href": "Clustering.html#results",
    "title": "Clustering",
    "section": "Results",
    "text": "Results\nAfter hyperparameter tuning was complete, I was able to perform both methods of clustering on the data and compare the results.\n\nClustering the data: K-Means\nUsing the number of clusters value of three, I trained a clustering model on my dataset and found the following clusters were formed:\n\n\nCode\nmodel=sklearn.cluster.KMeans(n_clusters=3).fit(df_embedded)\nlabels=model.predict(df_embedded)\n\nfig, ax = plt.subplots()\nax.scatter(df_embedded[:,0], df_embedded[:,1],c=labels, alpha=0.5) \nax.set(xlabel='Feature-1', ylabel='Feature-2',\ntitle='Cluster data')\nax.grid()\nplt.show()\n\n\n/Users/gracehyland/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\n\n\n\n\n\nClustering the data: Agglomerative Clustering\nI then used the optimal hyperparameter nmax=3 for agglomerative clustering and found the following clusters as a result:\n\n\nCode\nmodel = sklearn.cluster.AgglomerativeClustering(n_clusters=3).fit(df_embedded)\nlabels=model.labels_\n\nfig, ax = plt.subplots()\nax.scatter(df_embedded[:,0], df_embedded[:,1],c=labels, alpha=0.5) \nax.set(xlabel='Feature-1', ylabel='Feature-2',\ntitle='Cluster data')\nax.grid()\nplt.show()\n\n\n\n\n\nInterestingly, both K-Means and Agglomerative Clustering (after hyperparameter) tuning produce the same number of clusters and the assigned clusters are identical.\n\n\nFeatures Plotted with Target Labels\nAfter comparing the two clustering methods to each other I decided to compare the clusters formed by the models to categorical point value labels that represent regular season success.\n\n\nCode\ncolors = {'under80':'tab:blue', 'mid_tier':'tab:orange', 'over100':'tab:green'}\n\nplt.scatter(labeled_features['feature_1'],labeled_features['feature_2'],c=labeled_features['label'].map(colors))\nplt.xlabel(xlabel=\"Feature 1\")\nplt.ylabel(ylabel=\"Feature 2\")\nplt.show()\n\n\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n\n\n\n\n\nThese three groupings do not match up with the clusters formed by the models. The blue points on the scatter plot representing teams that finished the season with 80 points or fewer are clustered together towards the top right corner, but the teams that finished the season between 80 and 100 (orange) and the teams that finished with over 100 points (green) are pretty interspersed with one another."
  },
  {
    "objectID": "Clustering.html#conclusions",
    "href": "Clustering.html#conclusions",
    "title": "Clustering",
    "section": "Conclusions",
    "text": "Conclusions\nBased on the clusters formed by K-Means and Agglomerative clustering, the set of data that includes all 32 NHL teams during the 2022-2023 season and their team Corsi, Expected Goals, Hits, Takewaways, Giveaways, Faceoff Wins, Goals, and Shot Attempts can be readily grouped into distinct categories.\nBased on hyperparameter tuning, I can conclude that the groupings that make the most sense and keep the most similar teams together results in three different categories. This lines up with the hypothesis I had before clustering about there being three categories of teams by the end of the regular season: playoff teams, teams that were in playoff contention, and teams that were never really in playoff contention.\nHowever, the clusters that are formed from unsupervised learning with the data were pretty different from the labels I added to the data that categorized them into final standings points ranges. The teams that had a very low number of points (under 80) did cluster together somewhat on the graph as noted above, but the ranges of 80-100 points and over 100 points did not have any clear distinctions based on the features used.\nThis may mean that teams that make the playoffs and teams that just miss the playoffs are more similar than they are different. Alternatively, different segmentation of the final points standings for the labels may result in clearer groupings meaning teams that have point values in the 80s may be very similar to each other but not to teams with point values in the 90s."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hello everyone! My name is Grace Hyland and I am currently a first year graduate student at Georgetown University in the Data Science and Analytics program. I am completing this degree through the accelerated BS/MS program meaning I am a senior undergraduate student completing my coursework for a Bachelor’s of Science in Biology of Global Health as well while beginning my course towards a Master of Science in Data Science and Analytics.\nAs an undergraduate student at Georgetown University I have experience as a Teaching Assistant for foundational level courses in the Biology Department and have gained many important leadership skills from serving on the Executive Board of my sorority Kappa Alpha Theta for two years.\nIn my free time you can usually find me out for a run, exploring coffee shops with friends, or watching any sports event that is on. My interests in life vary and change a lot so I am always excited by the idea of having the data science tools to explore any topic that’s currently holding my attention in a detailed manner.\n\n\n\n\n\n\nEducation\nM.S. Data Science and Analytics, Georgetown University (Expected May 2025)\nB.S. Biology of Global Health, Georgetown University (Expected, May 2024)\n\n\nContact Me\nEmail: grace.hyland7@gmail.com\nLinkedin: https://www.linkedin.com/in/grace-hyland-1b2638205/"
  },
  {
    "objectID": "exploratory_data_analysis.html",
    "href": "exploratory_data_analysis.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "To begin exploring the central question of how power play play relates to the success of a hockey team in the NHL, I looked at data on different metrics for each player in the 2022 season. For each player the value of each evaluation metric is separated into a metric for the time played regular 5 on 5, a metric for the time played on a power play, a metric for the time played on a penalty kill, and a metric for the time played in situations that do not fit into any of these categories. The data used only contained players that reached a certain threshold for total playing time as explained in the Data Cleaning tab. The data was separated into power play and regular play data frames so that the regular play data set includes the value of each metric for every player during regular play and the power play data set includes the value of each metric for every player during power play minutes.\n\n\nCode\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nregular_data &lt;- read.csv(\"../data/01-modified-data/skaters_clean.csv\")\npp_data &lt;- read.csv(\"../data/01-modified-data/skaters_pp.csv\")\n\n\nAfter importing the datasets, I selected nine variables (expectedGoals, Corsi, shotAttempts, points, goals, faceoffsWon, takeaways, giveaways, and hits) to look at summary statistics for. I chose these seven variables because they are different from each other and correspond to noticeable moments on the ice. I first looked at summary statistics for regular play which can be seen below.\n\n\nCode\nsummary_data_regular &lt;- regular_data %&gt;%\n  subset(select=c(expectedGoals,corsi,shotAttempts,points,goals,faceoffsWon,takeways,giveaways,hits)) %&gt;%\n  summary()\nsummary_data_regular\n\n\n expectedGoals        corsi         shotAttempts       points     \n Min.   :0.3200   Min.   :0.3800   Min.   : 45.0   Min.   : 2.00  \n 1st Qu.:0.4600   1st Qu.:0.4700   1st Qu.:128.0   1st Qu.:14.00  \n Median :0.5000   Median :0.5000   Median :173.0   Median :20.00  \n Mean   :0.4981   Mean   :0.4983   Mean   :181.3   Mean   :22.38  \n 3rd Qu.:0.5300   3rd Qu.:0.5300   3rd Qu.:219.0   3rd Qu.:29.00  \n Max.   :0.6400   Max.   :0.6500   Max.   :443.0   Max.   :65.00  \n     goals         faceoffsWon        takeways       giveaways    \n Min.   : 0.000   Min.   :  0.00   Min.   : 1.00   Min.   : 2.00  \n 1st Qu.: 4.000   1st Qu.:  0.00   1st Qu.:15.00   1st Qu.:16.00  \n Median : 7.000   Median :  4.00   Median :22.00   Median :25.00  \n Mean   : 8.376   Mean   : 89.22   Mean   :24.51   Mean   :26.92  \n 3rd Qu.:12.000   3rd Qu.:113.50   3rd Qu.:32.00   3rd Qu.:35.00  \n Max.   :34.000   Max.   :739.00   Max.   :77.00   Max.   :89.00  \n      hits      \n Min.   :  5.0  \n 1st Qu.: 43.0  \n Median : 69.0  \n Mean   : 81.2  \n 3rd Qu.:108.0  \n Max.   :302.0  \n\n\nBefore creating summary statistics for the power play data I found the distribution of ice time minutes and removed players who were in the bottom 25% of players by icetime during power plays.\n\n\nCode\nsummary(pp_data$icetime)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      1     414    4313    5874   10288   19733 \n\n\nCode\n#remove bottom 25% of players by ice time\npp_data &lt;- pp_data %&gt;%\n  filter(icetime &gt; 414)\n\n\nTypically, a team will have two set power play units (totaling 10 players) that play the entire power play so I sought to exclude players who may have played a couple of shifts throughout the season but were not part of the main power play strategy.\n\n\nCode\n#summarize all metrics\nsummary_data_pp &lt;- pp_data %&gt;%\n  subset(select=c(expectedGoals,corsi,shotAttempts,points,goals,faceoffsWon,takeways,giveaways,hits)) %&gt;%\n  summary()\nsummary_data_pp\n\n\n expectedGoals        corsi         shotAttempts       points      \n Min.   :0.2400   Min.   :0.6000   Min.   :  0.0   Min.   : 0.000  \n 1st Qu.:0.8200   1st Qu.:0.8300   1st Qu.:  9.0   1st Qu.: 1.000  \n Median :0.8700   Median :0.8600   Median : 34.0   Median : 7.000  \n Mean   :0.8502   Mean   :0.8509   Mean   : 43.8   Mean   : 9.434  \n 3rd Qu.:0.9000   3rd Qu.:0.8800   3rd Qu.: 67.0   3rd Qu.:14.000  \n Max.   :1.0000   Max.   :1.0000   Max.   :208.0   Max.   :64.000  \n     goals         faceoffsWon       takeways        giveaways     \n Min.   : 0.000   Min.   :  0.0   Min.   : 0.000   Min.   : 0.000  \n 1st Qu.: 0.000   1st Qu.:  0.0   1st Qu.: 0.000   1st Qu.: 1.000  \n Median : 2.000   Median :  1.0   Median : 1.000   Median : 3.000  \n Mean   : 3.226   Mean   : 15.4   Mean   : 1.262   Mean   : 3.941  \n 3rd Qu.: 5.000   3rd Qu.: 10.0   3rd Qu.: 2.000   3rd Qu.: 6.000  \n Max.   :30.000   Max.   :195.0   Max.   :10.000   Max.   :27.000  \n      hits       \n Min.   : 0.000  \n 1st Qu.: 0.000  \n Median : 1.000  \n Mean   : 1.879  \n 3rd Qu.: 3.000  \n Max.   :15.000  \n\n\n\n\nAll seven of the variable that we are looking at in this analysis (expected goals, Corsi, shot attempts, points, goals, face offs won, takeaways, giveaways, and hits) have the distribution of values skewed higher when looking at data from power play minutes. This is an intuitive and expected result for many of the metrics related to taking shots because having a one man advantage does mean teams on power plays are more offensively motivated and focused. This is more surprising for the variables like takeaways, giveaways, and hits which are not explicitly offensive stats. The increase in value for these variables implies that there may be more action (and essentially more hockey) occurring during power play minutes than during other minutes during a game."
  },
  {
    "objectID": "exploratory_data_analysis.html#summary-statistics",
    "href": "exploratory_data_analysis.html#summary-statistics",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "To begin exploring the central question of how power play play relates to the success of a hockey team in the NHL, I looked at data on different metrics for each player in the 2022 season. For each player the value of each evaluation metric is separated into a metric for the time played regular 5 on 5, a metric for the time played on a power play, a metric for the time played on a penalty kill, and a metric for the time played in situations that do not fit into any of these categories. The data used only contained players that reached a certain threshold for total playing time as explained in the Data Cleaning tab. The data was separated into power play and regular play data frames so that the regular play data set includes the value of each metric for every player during regular play and the power play data set includes the value of each metric for every player during power play minutes.\n\n\nCode\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nregular_data &lt;- read.csv(\"../data/01-modified-data/skaters_clean.csv\")\npp_data &lt;- read.csv(\"../data/01-modified-data/skaters_pp.csv\")\n\n\nAfter importing the datasets, I selected nine variables (expectedGoals, Corsi, shotAttempts, points, goals, faceoffsWon, takeaways, giveaways, and hits) to look at summary statistics for. I chose these seven variables because they are different from each other and correspond to noticeable moments on the ice. I first looked at summary statistics for regular play which can be seen below.\n\n\nCode\nsummary_data_regular &lt;- regular_data %&gt;%\n  subset(select=c(expectedGoals,corsi,shotAttempts,points,goals,faceoffsWon,takeways,giveaways,hits)) %&gt;%\n  summary()\nsummary_data_regular\n\n\n expectedGoals        corsi         shotAttempts       points     \n Min.   :0.3200   Min.   :0.3800   Min.   : 45.0   Min.   : 2.00  \n 1st Qu.:0.4600   1st Qu.:0.4700   1st Qu.:128.0   1st Qu.:14.00  \n Median :0.5000   Median :0.5000   Median :173.0   Median :20.00  \n Mean   :0.4981   Mean   :0.4983   Mean   :181.3   Mean   :22.38  \n 3rd Qu.:0.5300   3rd Qu.:0.5300   3rd Qu.:219.0   3rd Qu.:29.00  \n Max.   :0.6400   Max.   :0.6500   Max.   :443.0   Max.   :65.00  \n     goals         faceoffsWon        takeways       giveaways    \n Min.   : 0.000   Min.   :  0.00   Min.   : 1.00   Min.   : 2.00  \n 1st Qu.: 4.000   1st Qu.:  0.00   1st Qu.:15.00   1st Qu.:16.00  \n Median : 7.000   Median :  4.00   Median :22.00   Median :25.00  \n Mean   : 8.376   Mean   : 89.22   Mean   :24.51   Mean   :26.92  \n 3rd Qu.:12.000   3rd Qu.:113.50   3rd Qu.:32.00   3rd Qu.:35.00  \n Max.   :34.000   Max.   :739.00   Max.   :77.00   Max.   :89.00  \n      hits      \n Min.   :  5.0  \n 1st Qu.: 43.0  \n Median : 69.0  \n Mean   : 81.2  \n 3rd Qu.:108.0  \n Max.   :302.0  \n\n\nBefore creating summary statistics for the power play data I found the distribution of ice time minutes and removed players who were in the bottom 25% of players by icetime during power plays.\n\n\nCode\nsummary(pp_data$icetime)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      1     414    4313    5874   10288   19733 \n\n\nCode\n#remove bottom 25% of players by ice time\npp_data &lt;- pp_data %&gt;%\n  filter(icetime &gt; 414)\n\n\nTypically, a team will have two set power play units (totaling 10 players) that play the entire power play so I sought to exclude players who may have played a couple of shifts throughout the season but were not part of the main power play strategy.\n\n\nCode\n#summarize all metrics\nsummary_data_pp &lt;- pp_data %&gt;%\n  subset(select=c(expectedGoals,corsi,shotAttempts,points,goals,faceoffsWon,takeways,giveaways,hits)) %&gt;%\n  summary()\nsummary_data_pp\n\n\n expectedGoals        corsi         shotAttempts       points      \n Min.   :0.2400   Min.   :0.6000   Min.   :  0.0   Min.   : 0.000  \n 1st Qu.:0.8200   1st Qu.:0.8300   1st Qu.:  9.0   1st Qu.: 1.000  \n Median :0.8700   Median :0.8600   Median : 34.0   Median : 7.000  \n Mean   :0.8502   Mean   :0.8509   Mean   : 43.8   Mean   : 9.434  \n 3rd Qu.:0.9000   3rd Qu.:0.8800   3rd Qu.: 67.0   3rd Qu.:14.000  \n Max.   :1.0000   Max.   :1.0000   Max.   :208.0   Max.   :64.000  \n     goals         faceoffsWon       takeways        giveaways     \n Min.   : 0.000   Min.   :  0.0   Min.   : 0.000   Min.   : 0.000  \n 1st Qu.: 0.000   1st Qu.:  0.0   1st Qu.: 0.000   1st Qu.: 1.000  \n Median : 2.000   Median :  1.0   Median : 1.000   Median : 3.000  \n Mean   : 3.226   Mean   : 15.4   Mean   : 1.262   Mean   : 3.941  \n 3rd Qu.: 5.000   3rd Qu.: 10.0   3rd Qu.: 2.000   3rd Qu.: 6.000  \n Max.   :30.000   Max.   :195.0   Max.   :10.000   Max.   :27.000  \n      hits       \n Min.   : 0.000  \n 1st Qu.: 0.000  \n Median : 1.000  \n Mean   : 1.879  \n 3rd Qu.: 3.000  \n Max.   :15.000  \n\n\n\n\nAll seven of the variable that we are looking at in this analysis (expected goals, Corsi, shot attempts, points, goals, face offs won, takeaways, giveaways, and hits) have the distribution of values skewed higher when looking at data from power play minutes. This is an intuitive and expected result for many of the metrics related to taking shots because having a one man advantage does mean teams on power plays are more offensively motivated and focused. This is more surprising for the variables like takeaways, giveaways, and hits which are not explicitly offensive stats. The increase in value for these variables implies that there may be more action (and essentially more hockey) occurring during power play minutes than during other minutes during a game."
  },
  {
    "objectID": "exploratory_data_analysis.html#normalization-of-data",
    "href": "exploratory_data_analysis.html#normalization-of-data",
    "title": "Exploratory Data Analysis",
    "section": "Normalization of Data",
    "text": "Normalization of Data\nThe seven variables we are looking at all have different scales with some metrics using values between 0 and 1 to represent percentages (like Corsi) and other variables simply being a count of occurrences (like hits and takeaways). To better compare the variables to each other and to regular season success, I normalized all of the variables by dividing each value by the mean so that they are on the same scale.\n\n\nCode\nnorm_regular_data&lt;-mutate(regular_data, expectedGoals=expectedGoals/mean(expectedGoals), corsi=corsi/mean(corsi), shotAttempts=shotAttempts/mean(shotAttempts), points=points/mean(points), goals=goals/mean(goals), faceoffsWon=faceoffsWon/mean(faceoffsWon),takeways=takeways/mean(takeways),giveaways=giveaways/mean(giveaways),hits=hits/mean(hits))\n\nnorm_pp_data&lt;-mutate(pp_data, expectedGoals=expectedGoals/mean(expectedGoals), corsi=corsi/mean(corsi), shotAttempts=shotAttempts/mean(shotAttempts), points=points/mean(points), goals=goals/mean(goals), faceoffsWon=faceoffsWon/mean(faceoffsWon),takeways=takeways/mean(takeways),giveaways=giveaways/mean(giveaways),hits=hits/mean(hits))"
  },
  {
    "objectID": "exploratory_data_analysis.html#check-for-outliers",
    "href": "exploratory_data_analysis.html#check-for-outliers",
    "title": "Exploratory Data Analysis",
    "section": "Check for Outliers",
    "text": "Check for Outliers\nThe last step of data processing before looking at some visualizations to understand the data is checking for outliers.\n\n\nCode\nnorm_regular_data&lt;-norm_regular_data %&gt;%\n  subset(select=c(ID, name, team, expectedGoals,corsi,shotAttempts,points,goals,faceoffsWon,takeways,giveaways,hits))\nsapply(norm_regular_data, function(x) sum(x &gt; 3)) \n\n\n           ID          name          team expectedGoals         corsi \n          615           615           615             0             0 \n shotAttempts        points         goals   faceoffsWon      takeways \n            0             0             9            95             2 \n    giveaways          hits \n            3             8 \n\n\nCode\n#faceoffs won has a lot of outliers\n\n\nFor regular play data, most variables had fewer than 10 outliers out of 615 observations, but faceoffsWon has 95 observations that fall outside of 3 standard deviations from the mean. There were no extreme values that seemed impossible in the original dataset, so the outliers are not likely to be from a case of measuring and recording incorrectly so I did not decide to remove or replace them.\n\n\nCode\nnorm_pp_data&lt;-norm_pp_data %&gt;%\n  subset(select=c(ID, name, team, expectedGoals,corsi,shotAttempts,points,goals,faceoffsWon,takeways,giveaways,hits))\nsapply(norm_pp_data, function(x) sum(x &gt; 3))  \n\n\n           ID          name          team expectedGoals         corsi \n          461           461           461             0             0 \n shotAttempts        points         goals   faceoffsWon      takeways \n           17            19            38            54            47 \n    giveaways          hits \n           23            28 \n\n\nCode\n#most metrics have larger number of outliers for powerplays than regular play\n\n\nFor power play data, the number of outliers in most variables is higher than for the corresponding variable in the regular play data. This indicates that there are more extremes during power plays than during regular play. Again, I did not remove or replace outliers because they do not seem to be recording errors and may be informative for answering questions."
  },
  {
    "objectID": "exploratory_data_analysis.html#compare-regular-play-and-powerplays",
    "href": "exploratory_data_analysis.html#compare-regular-play-and-powerplays",
    "title": "Exploratory Data Analysis",
    "section": "Compare Regular Play and Powerplays",
    "text": "Compare Regular Play and Powerplays\n\n\nCode\n#join pp data with regular data\ncolnames(norm_pp_data) &lt;- c(\"ID\",\"name\",\"team\",\"expectedGoals_pp\",\"corsi_pp\",\"shotAttempts_pp\",\"points_pp\",\"goals_pp\",\"faceoffsWon_pp\",\"takeaways_pp\",\"giveaways_pp\",\"hits_pp\")\ndf.1 &lt;- left_join(norm_pp_data, norm_regular_data, by=\"ID\")\n\n\n\n\nCode\n#compare metric values between regular play and powerplays for all players\n\nlibrary(ggplot2)\nlibrary(gridExtra)\n\n\n\nAttaching package: 'gridExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\n\nCode\ng1&lt;-ggplot(data=df.1) + geom_point(aes(x=expectedGoals, y=expectedGoals_pp), color=\"purple\") + theme_bw() + labs(title=\"Expected Goals Percentage\", x=\"Regular Play\", y=\"Power Play\")\n\ng2&lt;-ggplot(data=df.1) + geom_point(aes(x=corsi, y=corsi_pp), color=\"purple\") + theme_bw() + labs(title=\"Corsi Percentage\", x=\"Regular Play\", y=\"Power Play\")\n\ng3&lt;-ggplot(data=df.1) + geom_point(aes(x=shotAttempts, y= shotAttempts_pp), color=\"purple\") + theme_bw() + labs(title=\"Shot Attempts\", x=\"Regular Play\", y=\"Power Play\")\n\ng4&lt;-ggplot(data=df.1) + geom_point(aes(x=points, y=points_pp), color=\"purple\") + theme_bw() + labs(title=\"Points\", x=\"Regular Play\", y=\"Power Play\")\n\ng5&lt;-ggplot(data=df.1) + geom_point(aes(x=goals, y=goals_pp), color=\"purple\") + theme_bw() + labs(title=\"Goals\", x=\"Regular Play\", y=\"Power Play\")\n\ng6&lt;-ggplot(data=df.1) + geom_point(aes(x=faceoffsWon, y=faceoffsWon_pp), color=\"purple\") + theme_bw() + labs(title=\"Faceoffs Won\", x=\"Regular Play\", y=\"Power Play\")\n\ng7&lt;-ggplot(data=df.1) + geom_point(aes(x=takeways, y=takeaways_pp), color=\"purple\") + theme_bw() + labs(title=\"Takeaways\", x=\"Regular Play\", y=\"Power Play\")\n\ng8&lt;-ggplot(data=df.1) + geom_point(aes(x=giveaways, y=giveaways_pp), color=\"purple\") + theme_bw() + labs(title=\"Giveaways\", x=\"Regular Play\", y=\"Power Play\")\n\ng9&lt;-ggplot(data=df.1) + geom_point(aes(x=hits, y=hits_pp), color=\"purple\") + theme_bw() + labs(title=\"Hits\", x=\"Regular Play\", y=\"Power Play\")\n\ngrid.arrange(g1,g2,g3,g4,g5,g6,g7,g8,g9, ncol=3)\n\n\n\n\n\nLooking at the scatterplots for each variable it seems that some of the metrics are relatively correlated between the power play data and the regular play data. The variables that seem the most linearly correlated are shot attempts, points, and goals. Shot attempts, points and goals are some of the earliest recorded metrics in ice hockey and are very easy for a casual viewer to track and understand. The more advanced metrics such as Corsi and Expected goals percent and less commonly tracked metrics such as hits and takeaways seem to be less correlated between power play minutes and regular minutes for the same players.\n\nHeat map of Correlation between Variable\n\n\nCode\nlibrary(reshape2)\n\n\n\nAttaching package: 'reshape2'\n\n\nThe following object is masked from 'package:tidyr':\n\n    smiths\n\n\nCode\ndf.2 &lt;- subset(df.1, select=-c(ID, name.x, team.x,name.y,team.y))\ncorr_matrix &lt;- round(cor(df.2),2)\ncorr_matrix &lt;- melt(corr_matrix)\n\n\nggplot(data = corr_matrix, aes(x=Var1, y=Var2, fill=value)) + geom_tile() + theme(axis.text.x = element_text(angle = 45, vjust = 0.5, hjust=1))\n\n\n\n\n\nThe heatmap of correlation indexes between each of the variables show that faceoffs won in regular play and faceoffs won in power plays are fairly strongly correlated, however a lot of the other metrics have a much lower correlation values like expected goals and Corsi."
  },
  {
    "objectID": "exploratory_data_analysis.html#compare-regular-play-and-penalty-kills",
    "href": "exploratory_data_analysis.html#compare-regular-play-and-penalty-kills",
    "title": "Exploratory Data Analysis",
    "section": "Compare Regular Play and Penalty Kills",
    "text": "Compare Regular Play and Penalty Kills\n\n\nCode\n#join pk data with regular data\ncolnames(norm_pk_data) &lt;- c(\"ID\",\"name\",\"team\",\"expectedGoals_pk\",\"corsi_pk\",\"shotAttempts_pk\",\"points_pk\",\"goals_pk\",\"faceoffsWon_pk\",\"takeaways_pk\",\"giveaways_pk\",\"hits_pk\")\ndf.2 &lt;- left_join(norm_pk_data, norm_regular_data, by=\"ID\")\n\n\n\n\nCode\n#compare metric values between regular play and penalty kills for all players\n\nlibrary(ggplot2)\n\nggplot(data=df.2) + geom_point(aes(x=expectedGoals, y=expectedGoals_pk))\n\n\n\n\n\nCode\nggplot(data=df.2) + geom_point(aes(x=corsi, y=corsi_pk))\n\n\n\n\n\nCode\nggplot(data=df.2) + geom_point(aes(x=shotAttempts, y= shotAttempts_pk))\n\n\n\n\n\nCode\nggplot(data=df.2) + geom_point(aes(x=points, y=points_pk))\n\n\n\n\n\nCode\nggplot(data=df.2) + geom_point(aes(x=goals, y=goals_pk))\n\n\n\n\n\nCode\nggplot(data=df.2) + geom_point(aes(x=faceoffsWon, y=faceoffsWon_pk))\n\n\n\n\n\nCode\nggplot(data=df.2) + geom_point(aes(x=takeways, y=takeaways_pk))\n\n\n\n\n\nCode\nggplot(data=df.2) + geom_point(aes(x=giveaways, y=giveaways_pk))\n\n\n\n\n\nCode\nggplot(data=df.2) + geom_point(aes(x=hits, y=hits_pk))"
  },
  {
    "objectID": "exploratory_data_analysis.html#compare-powerplay-and-penalty-kill-success-to-team-record",
    "href": "exploratory_data_analysis.html#compare-powerplay-and-penalty-kill-success-to-team-record",
    "title": "Exploratory Data Analysis",
    "section": "Compare Powerplay and Penalty Kill Success to Team Record",
    "text": "Compare Powerplay and Penalty Kill Success to Team Record\n\nFind powerplay and penalty kill success rates for all teams in 2022 season\nFind data with points record for all teams in 2022 season\nAdd variable for each team that indicates whether they made the playoffs\nFind correlation between pp and pk success and team record\n\n\n\nCode\n#load team data and team record data\nteam_data &lt;- read.csv(\"../data/01-modified-data/team_record_clean.csv\")\n\n\n\n\nCode\n#compare powerplay success to overall season success\nggplot(data=team_data) + geom_point(aes(x=ppPercent, y=points))\n\n\n\n\n\nCode\nggplot(data=team_data) + geom_point(aes(x=ppGoals, y=points))\n\n\n\n\n\nCode\n#compare penalty kill success to overall season success\nggplot(data=team_data) + geom_point(aes(x=pkPercent, y=points))\n\n\n\n\n\nCode\nggplot(data=team_data) + geom_point(aes(x=pkGoalsAgainst, y=points))"
  },
  {
    "objectID": "decision-trees.html",
    "href": "decision-trees.html",
    "title": "Decision Trees",
    "section": "",
    "text": "Decision trees are a supervised learning model which can be used for either classification or regression. A decision tree is a model that follows a certain sequence of rules which form many different paths creating a branching structure that can be followed down from the top to the bottom to classify a data point with certain feautures into one of the available labels. The process starts with a root node that is alone at the top of the branching structure. At the root node, a rule is defined that will provide one of two outcomes for any given data point based on its features. Each of the two outcomes corresponds to one of the branching paths. Each path will lead to another node that will again have a defined rule that splits the data into one of two pathways. This process continues for any number of nodes until a terminal node is reached where one of the two outcomes from the rule is assigning a value of the target variable.\nDecision tree models are trained on a labelled set of training data and without defined hyperparameters they will split all of the given training data into nodes until all of the terminal nodes are pure meaning all of the data points at that node have the same value of the target variable. If there is a mix of different target variables values than the node is called impure. When the decision tree is being formed, a good split of the data is one where the two nodes the data is split into have purer class distribution. The purity of the node can be measured using a metric called the Gini Impurity score. Gini Impurity takes on a value between 0 and 0.5. The Gini Impurity for the split is calculated by taking a weighted average of the Gini Impurities of the datasets at the leaves. The best split is determined by calculating the gain for each split option. The gain is caluclated by subtracting the Gini Impurity after the data is split from the Gini impurity of all of the data before splitting.\nDecision trees are relatively prone to overfitting, but pruning by hyperparameter tuning can be used to resolve this issue. Pruning reduces the size of the tree through several different methods reducing the complexity of the tree. One hyperparameter that can be adjusted is the maximum number of layers. The maximum number of the layers is the number of nodes the tree can have before it will stop being split. Another one is setting a minimum number of samples that is required to split a node. Both of these methods of pruning help combat overfitting to improve predictive accuracy by removing sections of the trees that are less important of redundant."
  },
  {
    "objectID": "decision-trees.html#class-distribution",
    "href": "decision-trees.html#class-distribution",
    "title": "Decision Trees",
    "section": "Class Distribution",
    "text": "Class Distribution\nTo start the process of building a decision tree that will classify NHL teams into playoff teams or nonplayoff teams based on expected goals for, corsi, shot attempts, expected goals for, faceoffs won, takeaways, giveaways, and hits I loaded in my normalized and labelled data and determined the distribution of each type of team within the five seasons of team data.\n\n\nCode\nimport numpy as np\nimport pandas as pd\n\n\n\n\nCode\ndf=pd.read_csv(\"../data/01-modified-data/team_pp_data_clean.csv\")\nlabel = df['playoff'].copy()\ndistribution=df.groupby('playoff').size()\ndf=df.drop(columns=['Unnamed: 0','playoff'])\nfeatures=df.columns\nfeature_matrix = df[features].copy()\nprint(\"Frequency of each value of playoff variable: \",distribution)\n\n\nFrequency of each value of playoff variable:  playoff\n0    62\n1    64\ndtype: int64\n\n\nThe split of the dataset into playoff or non-playoff team is almost an even split."
  },
  {
    "objectID": "decision-trees.html#baseline-model",
    "href": "decision-trees.html#baseline-model",
    "title": "Decision Trees",
    "section": "Baseline Model",
    "text": "Baseline Model\nAs a baseline model to compare the results of the decision tree to, I ran a random classifier function on the data.\n\n\nCode\nimport random\nfrom collections import Counter\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_recall_fscore_support\n\ndef random_classifier(y_data):\n    ypred=[];\n    max_label=np.max(y_data); \n    for i in range(0,len(y_data)):\n        ypred.append(int(np.floor((max_label+1)*np.random.uniform(0,1))))\n\n    print(\"-----RANDOM CLASSIFIER-----\")\n    print(\"count of prediction:\",Counter(ypred).values()) # counts the elements' frequency\n    print(\"probability of prediction:\",np.fromiter(Counter(ypred).values(), dtype=float)/len(y_data)) # counts the elements' frequency\n    print(\"accuracy\",accuracy_score(y_data, ypred))\n    print(\"percision, recall, fscore,\",precision_recall_fscore_support(y_data, ypred))\n\n\n\n\nCode\nrandom_classifier(label)\n\n\n-----RANDOM CLASSIFIER-----\ncount of prediction: dict_values([66, 60])\nprobability of prediction: [0.52380952 0.47619048]\naccuracy 0.5555555555555556\npercision, recall, fscore, (array([0.55      , 0.56060606]), array([0.53225806, 0.578125  ]), array([0.54098361, 0.56923077]), array([62, 64]))\n\n\nThe random classifier has a relatively high accuracy score of around 0.5 indicating that assigning the class label randomly has a 1 in 2 shot of being accurate. This is pretty good for a random classifier, maybe because the class distribution in the data is 50-50, but using a good decision tree should provide better results."
  },
  {
    "objectID": "decision-trees.html#model-tuning",
    "href": "decision-trees.html#model-tuning",
    "title": "Decision Trees",
    "section": "Model Tuning",
    "text": "Model Tuning\nAfter running the baseline random classifier model, I will run a decision tree model on the set of training data without modifying any of the hyperparameters.\n\n\nCode\nfrom sklearn.model_selection import train_test_split\n\nX = feature_matrix\ny= label\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size=0.2,\n    random_state=100\n)\n\n\n\n\nCode\nfrom sklearn import tree\nimport sklearn.metrics\nimport matplotlib.pyplot as plt\n\nmodel=tree.DecisionTreeClassifier()\nmodel=model.fit(X_train,y_train)\n\nyp_train = model.predict(X_train)\nyp_test = model.predict(X_test)\n\n#training data\nprint(\"training accuracy score:\",sklearn.metrics.accuracy_score(y_true=y_train, y_pred=yp_train))\nprint(\"training recall score:\",sklearn.metrics.recall_score(y_true=y_train, y_pred=yp_train))\nprint(\"training precision score:\",sklearn.metrics.precision_score(y_true=y_train, y_pred=yp_train))\nprint(\"training f score:\",sklearn.metrics.f1_score(y_true=y_train, y_pred=yp_train))\n\n#test data\nprint(\"test accuracy score:\",sklearn.metrics.accuracy_score(y_true=y_test, y_pred=yp_test))\nprint(\"test recall score:\",sklearn.metrics.recall_score(y_true=y_test, y_pred=yp_test))\nprint(\"test precision score:\",sklearn.metrics.precision_score(y_true=y_test, y_pred=yp_test))\nprint(\"test f score:\",sklearn.metrics.f1_score(y_true=y_test, y_pred=yp_test))\n\nfig = plt.figure(figsize=(25,20))\n__=tree.plot_tree(model, feature_names=list(features),filled=True)\n\n\ntraining accuracy score: 1.0\ntraining recall score: 1.0\ntraining precision score: 1.0\ntraining f score: 1.0\ntest accuracy score: 0.6153846153846154\ntest recall score: 0.625\ntest precision score: 0.7142857142857143\ntest f score: 0.6666666666666666\n\n\n\n\n\nThe baseline decision tree model resulted in an accuracy score of around 71% for the test data that had been partitioned from the original dataset. This is definitely an improvement over the baseline random classifer model, but the training data had an accuracy score of 100% which may indicate that overfitting is occuring. Another indication of over fitting is the very small number of samples in most of the terminal nodes on the decision tree. To adjust for overfitting, we can look at different values of the maximum number of layers and see which values results in the best balance of accuracy between training and test data. The number of layers that the baseline decision tree model used was 9, so I will evaluate the model accuracy with max_depth ranging from 0 to 10.\n\n\nCode\ntest_results=[]\ntrain_results=[]\n\nfor num_layer in range (1,10):\n    model=tree.DecisionTreeClassifier(max_depth=num_layer)\n    model=model.fit(X_train,y_train)\n\n    yp_train=model.predict(X_train)\n    yp_test=model.predict(X_test)\n    \n    test_results.append([num_layer,sklearn.metrics.accuracy_score(y_test, yp_test),sklearn.metrics.recall_score(y_test, yp_test),sklearn.metrics.precision_score(y_test, yp_test),sklearn.metrics.f1_score(y_test, yp_test)])\n    train_results.append([num_layer,sklearn.metrics.accuracy_score(y_train, yp_train),sklearn.metrics.recall_score(y_train, yp_train),sklearn.metrics.precision_score(y_train, yp_train),sklearn.metrics.f1_score(y_train, yp_train)])\n\n\ndf_test_results=pd.DataFrame(test_results)\ndf_test_results.columns=['num_layers','accuracy_score','recall_score','precision_score','f_score']\n\ndf_train_results=pd.DataFrame(train_results)\ndf_train_results.columns=['num_layers','accuracy_score','recall_score','precision_score','f_score']\n\ndfTuning=pd.merge(df_train_results,df_test_results,how=\"inner\",on='num_layers')\ndfTuning.columns=['num_layers','accuracy_score_tr','recall_score_tr','precision_score_tr','f_score_tr','accuracy_score_te','recall_score_te','precision_score_te','f_score_te']\n\nplt.plot(dfTuning['num_layers'],dfTuning['accuracy_score_tr'],label=\"train\")\nplt.plot(dfTuning['num_layers'],dfTuning['accuracy_score_te'],label=\"test\")\nplt.xlabel(xlabel=\"Number of layers in decision tree (max_depth)\")\nplt.ylabel(ylabel=\"Accuracy: Training (orange) and Test (blue)\")\nplt.legend()\nplt.show()\n\nplt.plot(dfTuning['num_layers'],dfTuning['recall_score_tr'],label=\"train\")\nplt.plot(dfTuning['num_layers'],dfTuning['recall_score_te'],label=\"test\")\nplt.xlabel(xlabel=\"Number of layers in decision tree (max_depth)\")\nplt.ylabel(ylabel=\"Recall Score: Training (orange) and Test (blue)\")\nplt.legend()\nplt.show()\n\nplt.plot(dfTuning['num_layers'],dfTuning['precision_score_tr'],label=\"train\")\nplt.plot(dfTuning['num_layers'],dfTuning['precision_score_te'],label=\"test\")\nplt.xlabel(xlabel=\"Number of layers in decision tree (max_depth)\")\nplt.ylabel(ylabel=\"Precision Score: Training (orange) and Test (blue)\")\nplt.legend()\nplt.show()\n\nplt.plot(dfTuning['num_layers'],dfTuning['f_score_tr'],label=\"train\")\nplt.plot(dfTuning['num_layers'],dfTuning['f_score_te'],label=\"test\")\nplt.xlabel(xlabel=\"Number of layers in decision tree (max_depth)\")\nplt.ylabel(ylabel=\"F1 Score: Training (orange) and Test (blue)\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbove are graphs showing various metrics for the quality of the model based on the number of layers. There metrics for the test data all seem to fluctaute around about the same value regardless of the number of layers, but appear to be the highest at maximum depth of 7."
  },
  {
    "objectID": "decision-trees.html#final-results",
    "href": "decision-trees.html#final-results",
    "title": "Decision Trees",
    "section": "Final Results",
    "text": "Final Results\n\n\nCode\nmodel = tree.DecisionTreeClassifier(max_depth=7)\nmodel = model.fit(X_train, y_train)\n\nyp_train=model.predict(X_train)\nyp_test=model.predict(X_test)\n\nfig = plt.figure(figsize=(25,20))\n__=tree.plot_tree(model, feature_names=list(features),filled=True)\n\n\n\n\n\n\n\nCode\n#training data\nprint(\"Training accuracy score:\",sklearn.metrics.accuracy_score(y_true=y_train, y_pred=yp_train))\nprint(\"Training recall score:\",sklearn.metrics.recall_score(y_true=y_train, y_pred=yp_train))\nprint(\"Training precision score:\",sklearn.metrics.precision_score(y_true=y_train, y_pred=yp_train))\nprint(\"Training f score:\",sklearn.metrics.f1_score(y_true=y_train, y_pred=yp_train))\n\n#test data\nprint(\"Test accuracy score:\",sklearn.metrics.accuracy_score(y_true=y_test, y_pred=yp_test))\nprint(\"Test recall score:\",sklearn.metrics.recall_score(y_true=y_test, y_pred=yp_test))\nprint(\"Test precision score:\",sklearn.metrics.precision_score(y_true=y_test, y_pred=yp_test))\nprint(\"Test f score:\",sklearn.metrics.f1_score(y_true=y_test, y_pred=yp_test))\n\n\nTraining accuracy score: 1.0\nTraining recall score: 1.0\nTraining precision score: 1.0\nTraining f score: 1.0\nTest accuracy score: 0.7307692307692307\nTest recall score: 0.6875\nTest precision score: 0.8461538461538461\nTest f score: 0.7586206896551724\n\n\nUsing the optimal value of maximum layers resulted in an accuracy score of 73% on the test data, however the accuracy score of the training data is still 100% so there is still some degree of overfitting. This may be able to be resolved by using data from a greater number of seasons in the original dataset.\n\n\nCode\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\ndef confusion_plot(y_data, y_pred):\n    cm=confusion_matrix(y_data,y_pred)\n    disp=ConfusionMatrixDisplay(confusion_matrix=cm)\n    disp.plot()\n    plt.show()\n\ncm=confusion_matrix(y_test, yp_test)\ndisp=ConfusionMatrixDisplay(confusion_matrix=cm)\ndisp.plot()\nplt.show()\n\n\n\n\n\nIn the above confusion matrix, the label 1 corresponds with teams that make the playoffs and 0 corresponds with teams who do not make the playoffs. As seen in the matrix, the model seems to be better at predicting teams that miss the playoffs; only two of the teams that made did not make the playoffs were predicted to be playoff teams. For teams that did make the playoffs, five out of sixteen were assigned the label of non-playoff team. The decision tree model had an even split (13-13) of the two assigned labels which aligns with the distribution of the two labels in the larger dataset, but the test data set had an actual distribution of 16 to 10."
  },
  {
    "objectID": "decision-trees.html#conclusions",
    "href": "decision-trees.html#conclusions",
    "title": "Decision Trees",
    "section": "Conclusions",
    "text": "Conclusions\nIn the optimally-fit decision tree model, the most common feature to split the data on is expected goals for. It is used much more frequently than any measure of defensive success of a team. This indicates that expected goals for may be a measure that really separates playoff teams from non-playoff teams in the NHL. Of note, this data is only from power play situations which are a heavily offensively driven game situation so investigation into whether this holds true for expected goals compared to defensive metrics in regular five on five play may reveal just how important high quality defense is and whether or not it can be made up for by strong offensive ability on powerplays.\nThe confusion matrix of the optimally-fit model indicates that this model has better predictive power for teams that did not make the playoffs than teams that did make the playoffs. This may indicate that teams that miss out on making the playoffs are more similar to one another than the group of teams that do make the playoffs. There may be a greater degree of variability in playoff teams with the very best teams that are at the top of their conferences having a larger gap over teams that squeak in in wildcard slots than wildcard slots have over teams that miss out. Further investigation by removing teams that clinch playoff spots or are eliminated a month before they start may provide more valuable insights in comparing teams that are fighting for playoff spots during the last couple of weeks. A model trained on this data may be able to pick up on more nuance because the teams will be more similar to each other when the extremes are excluded."
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "In recent decades, the time and attention put into understanding sports using data and modeling has greatly increased for all sports. The earliest adopting sport was baseball and some close followers have been American football and basketball. The sport of ice hockey took longer to adopt the practice relative to the others. One of the proposed reasons for this is that there are specific challenges that come with trying to collect and analyze data from hockey.\nThese challenges include the infrequency of scoring events, the continuity of game play, and frequent substitutions. Ice hockey is one of the lowest scoring sports, especially compared to the likes of football and basketball. It is not uncommon for games to have fewer than two or three total goals scored. Given this, it can be hard to quantify the impact of each player on the success of their team and the impact that an individual play can have on a game’s outcome without creating more advanced statistical evaluation tools. It is as a result of this fact that the game of hockey is rather continuous compared to the discrete nature of sports such as football and baseball. Without a high number of scoring events, play is much less frequently halted so it is more difficult to classify events as individual plays. Additionally, the lack of stoppages in play mean that players are substituted in the middle of plays and events. This in combination with the very short length of shifts (typically under a minute) contribute to the difficulty of isolating the impact of players on the results of the game.\nNonetheless, steps have been taken over the past two decades to improve the state of hockey analytics. At the beginning of the sport, the goals and assists by players were recorded by officials and coaches and for a long time that was the only data collected on the game. The earliest attempt at an advanced statistic in hockey was plus-minus. Plus-minus is calculated for each player by adding the goals scored for the team while the player is on the ice and subtracting the goals scored against the team while the player is on the ice. Plus-minus runs into the same issue as many statistics in hockey where it is difficult to separate individual players’ contributions and strengths from the overall strength of their team and their line because players are usually on the ice with the same people at the same time.\nAround the mid 2000s, people began to look beyond goals scored and allowed as the only data points that can be used to evaluate players. Some of the other factors considered are shots taken, penalty differential, quality of opponents, and the usage of players based on the zone location of the puck when they are deployed. One of the early popular evaluation metrics was Corsi. Corsi is calculated by taking the sum of shots on goal, missed shots, and blocked shots. In following years Corsi was modified to different iterations that account for some of the problems with the original metric such as Corsi relative (compares individual player to the rest of their team), Corsi close (only considers situations where the game is within one goal), and score adjusted Corsi (places shot attempts in the context of expectations based on the current score.) The major problem that remained after these adjustments is that the metric still doesn’t account for the quality of the shots taken.\nMore recent research into hockey analytics metrics have focused on shot quality and expected goals. Shot quality was first brought up in 2004 and was found by cataloguing shot types and whether or not they were rebounds. Goal probability curves were fit to the shot types. The expected goal metric comes from summing the total goal probabilities. More modern research has focused on improving the expected goals model. Brian Macdonald (2012) created an expected goals metric using least squares regression and ridge regression models. In 2015, Sprigings and Toumi made a model that incorporated shot type, shot angle, distance from net, whether the shooter was shooting off their strong side, and a value of the player’s shooting talent.\nThese new metrics have advanced analytics applications to the sport of hockey and have shown predictive power when looking at team and player success. However, often times a very important aspect of play in hockey is overlooked when applying these metrics: power plays and penalty kills.\nPower plays and penalty kills (together called special teams opportunities) are scenarios in which one team has an extra player on the ice compared to the other after a penalty. These scenarios vary a lot game to game which is why they are excluded from metrics, but they also are very important parts of the game outcome. About one third of goals scored in hockey games come from power plays (33.5% of goals in the olympics; 29% of goals in Finland’s hockey league). In this project I would like to explore the impact that power plays and penalty kills have on the outcome of games and seek answers to the question of how a coach should best approach special teams situations. I will look to address the following questions:"
  },
  {
    "objectID": "dimensionality-reduction.html",
    "href": "dimensionality-reduction.html",
    "title": "Dimensionality Reduction",
    "section": "",
    "text": "In following steps in this project, I will be creating clustering models and decision trees. Before doing this I aim to reduce the dimensions of the data frame from the eight features I have been using thus far, to a smaller number that facilitates better visualization and allows for quicker and easier computation while still maintaining enough predictive power.\nI will be implementing both Principal Component Analysis (PCA) and t-distributed Stochastic Neighbor Embedding (t-SNE). To do this, I will use PCA from sklearn.decomposition and TSNE from sklearn.manifold in Python."
  },
  {
    "objectID": "dimensionality-reduction.html#dimensionality-reduction-with-t-sne",
    "href": "dimensionality-reduction.html#dimensionality-reduction-with-t-sne",
    "title": "Dimensionality Reduction",
    "section": "Dimensionality Reduction with t-SNE",
    "text": "Dimensionality Reduction with t-SNE\nI next applied dimensionality reduction with t-SNE on the same data set. I used the standard number of components of two and varied the perplexity.\nGraphs of the dimensionality reduction with different perplexities as well as the shape of the data set formed and the first few points are shown below.\n\n\nCode\nfrom sklearn.manifold import TSNE\ndf1_embedded=TSNE(n_components=2,learning_rate='auto',init='random',perplexity=3).fit_transform(df)\nprint(\"perplexity: 3\")\nprint(\"shape: \",df1_embedded.shape)\nprint(\"First few points: \\n\",df1_embedded[0:4,:])\nprint(\"---------------------\")\n\ndf2_embedded=TSNE(n_components=2,learning_rate='auto',init='random',perplexity=15).fit_transform(df)\nprint(\"perplexity: 15\")\nprint(\"shape: \",df2_embedded.shape)\nprint(\"First few points: \\n\",df2_embedded[0:4,:])\nprint(\"---------------------\")\n\ndf3_embedded=TSNE(n_components=2,learning_rate='auto',init='random',perplexity=30).fit_transform(df)\nprint(\"perplexity: 30\")\nprint(\"shape: \",df3_embedded.shape)\nprint(\"First few points: \\n\",df3_embedded[0:4,:])\nprint(\"---------------------\")\n\ndf4_embedded=TSNE(n_components=2,learning_rate='auto',init='random',perplexity=45).fit_transform(df)\nprint(\"perplexity: 45\")\nprint(\"shape: \",df4_embedded.shape)\nprint(\"First few points: \\n\",df4_embedded[0:4,:])\nprint(\"---------------------\")\n\n\n\nfig, axs = plt.subplots(2, 2)\naxs[0,0].scatter(df1_embedded[:,0],df1_embedded[:,1],alpha=0.5)\naxs[0, 0].set_title('Perplexity: 3')\naxs[0,1].scatter(df2_embedded[:,0],df2_embedded[:,1],alpha=0.5)\naxs[0, 1].set_title('Perplexity: 15')\naxs[1,0].scatter(df3_embedded[:,0],df3_embedded[:,1],alpha=0.5)\naxs[1, 0].set_title('Perplexity: 30')\naxs[1,1].scatter(df4_embedded[:,0],df4_embedded[:,1],alpha=0.5)\naxs[1, 1].set_title('Perplexity: 45')\nfor ax in axs.flat:\n    ax.set(xlabel='Feature-1', ylabel='Feature-2')\nfor ax in axs.flat:\n    ax.label_outer()\nplt.show()\n\n\nperplexity: 3\nshape:  (126, 2)\nFirst few points: \n [[-48.405277   11.784265 ]\n [-25.97799   -19.775352 ]\n [ -2.8351245  15.392918 ]\n [ 38.675873   -7.1923904]]\n---------------------\nperplexity: 15\nshape:  (126, 2)\nFirst few points: \n [[-22.509907    -2.7635748 ]\n [-14.06056     -9.908393  ]\n [ -4.9173107   -0.06018139]\n [ 11.1908245    9.006622  ]]\n---------------------\nperplexity: 30\nshape:  (126, 2)\nFirst few points: \n [[-8.927938    3.6503627 ]\n [-5.711835    5.5288625 ]\n [-0.60401803  1.1408626 ]\n [ 4.474953   -1.2845267 ]]\n---------------------\nperplexity: 45\nshape:  (126, 2)\nFirst few points: \n [[ 2.1755915  3.0193036]\n [ 4.661678   0.5690624]\n [ 0.3941788 -1.819139 ]\n [-1.975171  -3.0735848]]\n---------------------\n\n\n\n\n\nThe lower perplexity values of 3 and 15 show the data forming more distinct clusters together. The perplexity three graph has many small clusters while the perplexity 15 graph appears to have two clear separate groups in the data. The higher perplexity values of 30 and 45 show more even distributions of the data.\nThe optimal perplexity to use when reducing the dimensions of this data set is likely around 15, because we want the differences in the data to be maintained."
  },
  {
    "objectID": "dimensionality-reduction.html#evaluation-and-comparison",
    "href": "dimensionality-reduction.html#evaluation-and-comparison",
    "title": "Dimensionality Reduction",
    "section": "Evaluation and Comparison",
    "text": "Evaluation and Comparison\ntSNE and PCA are two different ways of performing dimensionality reduction on a dataset with many features. Using PCA, we can select based on graphs the optimal number of features that the larger dataset should be reduced to and then carry out that reduction. In the case of this data, the optimal number of features to reduce to using PCA was 4. Using t-SNE, typically you are always reducing the two features. Reducing to two features as I did in t-SNE results in simpler visualizations of the distribution of the two features because it can be plotted on a simple 2 dimensional plane unlike the 4 feautures from PCA which had to be plotted against each other in different combinations across multiple plots."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About Me",
    "section": "",
    "text": "Hello everyone! My name is Grace Hyland and I am currently a first year graduate student at Georgetown University in the Data Science and Analytics program. I am completing this degree through the accelerated BS/MS program meaning I am a senior undergraduate student completing my coursework for a Bachelor’s of Science in Biology of Global Health as well while beginning my course towards a Master of Science in Data Science and Analytics.\nAs an undergraduate student at Georgetown University I have experience as a Teaching Assistant for foundational level courses in the Biology Department and have gained many important leadership skills from serving on the Executive Board of my sorority Kappa Alpha Theta for two years.\nIn my free time you can usually find me out for a run, exploring coffee shops with friends, or watching any sports event that is on. My interests in life vary and change a lot so I am always excited by the idea of having the data science tools to explore any topic that’s currently holding my attention in a detailed manner.\n\n\n\n\n\n\nEducation\nM.S. Data Science and Analytics, Georgetown University (Expected May 2025)\nB.S. Biology of Global Health, Georgetown University (Expected, May 2024)\n\n\nContact Me\nEmail: grace.hyland7@gmail.com\nLinkedin: https://www.linkedin.com/in/grace-hyland-1b2638205/"
  },
  {
    "objectID": "naive_bayes.html",
    "href": "naive_bayes.html",
    "title": "Introduction to Naive Bayes",
    "section": "",
    "text": "The Naive Bayes classification machine learning model uses Bayes Theorem to classify data points into one of several labels. When multiple features are used, the model calculates the probability of each label being the true label based on the conditional probability from the state of the feature. This is done for each of the features present, and the model assigns the label with the highest probability.\nThe Naive Bayes model operates under the assumption that the effects of the different features are independent of one another.\nGiven my prior hypothesis that certain team evaluation metrics during powerplays may be more predictive of regular season team success, I am looking to use Naive Bayes classification to determine which features are most predictive and then determine if these features during power play situations can be more accurately used to train a ML model to predict playoff berths than features from regular 5 on 5 game play."
  },
  {
    "objectID": "exploratory_data_analysis.html#compare-power-play-success-to-team-record",
    "href": "exploratory_data_analysis.html#compare-power-play-success-to-team-record",
    "title": "Exploratory Data Analysis",
    "section": "Compare Power play Success to Team Record",
    "text": "Compare Power play Success to Team Record\nNow that I have determined which variables are the most correlated for players between their time playing in regular play and their time playing during power plays, I want to evaluate just how important success on a power play is for a team. To do this I will be comparing success rate on power plays (which is defined as the percentage of team power plays which result in a goal) and the team’s record for the season. In the NHL, a teams standings within the league are determined by a value called points. Teams get 2 points for every game won in regulation or overtime/shootouts, 1 point for games lost in overtime/shootouts, and 0 points for games lost in regulation. To compare power play success to record, I am looking at a plot of team points against power play success rate.\n\n\nCode\n#load team data and team record data\nteam_data &lt;- read.csv(\"../data/01-modified-data/team_record_clean.csv\")\n\n\n\n\nCode\n#compare powerplay success to overall season success\nggplot(data=team_data) + geom_point(aes(x=ppPercent, y=points), color=\"purple\") + theme_bw() + labs(title=\"Regular Season Success Based on Power Play Success\", x=\"Power Play Percent\", y=\"Team Points\")\n\n\n\n\n\nThere appears to be a fairly significant correlation between power play success percentage and regular season team success. This highlights the importance of having strong players with strong metrics on the power play for a team to have success.\nIn addition to power play success, I compared the pure number of goals a team scores on the power play to their regular season team points. Using number of goals on power plays also accounts for the number of playoff opportunities a team gets throughout the season.\n\n\nCode\nggplot(data=team_data) + geom_point(aes(x=ppGoals, y=points), color=\"purple\") + theme_bw() + labs(title=\"Regular Season Success Based on Power Play Goals\", x=\"Power Play Goals\", y=\"Team Points\")\n\n\n\n\n\nThe scatter plot comparing regular season team points to number of goals scored on power plays is very similar to the above scatter plot of power play success percentage and regular season team success. This further emphasizes the importance of a strong power play for overall team success."
  },
  {
    "objectID": "naive_bayes.html#prepare-power-play-data-for-naive-bayes",
    "href": "naive_bayes.html#prepare-power-play-data-for-naive-bayes",
    "title": "Introduction to Naive Bayes",
    "section": "Prepare Power Play Data for Naive Bayes",
    "text": "Prepare Power Play Data for Naive Bayes\nI prepared the team powerplay data for Naive Bayes classification in an R file linked here. Team data from the 2018-2019, 2020-2021, 2021-2022, and 2022-2023 seasons were combined and the desired features were subsetted from the greater dataset. An additional variable was added for whether or not the team made the playoffs that season and all numerical metrics were normalized. Finally, all non-numerical variables except the playoff variable were removed.\n\nimport numpy as np\nimport pandas as pd\n\n\ndf=pd.read_csv(\"../data/01-modified-data/team_pp_data_clean.csv\")\nprint(df.shape)\n\nlabel = df['playoff'].copy()\ndf=df.drop(columns=['Unnamed: 0','playoff'])\nfeatures=df.columns\nfeature_matrix = df[features].copy()\n\n\n(126, 10)\n\n\n\nfrom sklearn.model_selection import train_test_split\n\nX = feature_matrix\ny= label\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size=0.2,\n    random_state=100\n)\n\nAfter loading the pre-prepared dataset, I used sklearn to split the dataset into a training set and a test set with an 80-20 split. It is important to keep a portion of the data aside to test the accuracy of the model on.\n\nFeature Selection\nFeature selection is a process by which only the most predictive and least correlated features are used to train a model in order to maximize accuracy.\nTo find the subset of features that result in the highest accuracy score, I will find every possible combination of the seven features and calculate a metric to evaluate their correlation coefficients.\n\nfrom itertools import chain, combinations\n\n#list all possible feature subsets\nfeature_subsets = list(features)\nfeature_subset=chain.from_iterable(combinations(feature_subsets,r) for r in range(len(feature_subsets)+1))\nfeature_subset=list(feature_subset)\nprint(feature_subset)\n\n\n[(), ('xGoalsPercentage',), ('corsiPercentage',), ('shotAttemptsFor',), ('goalsFor',), ('faceOffsWonFor',), ('takeawaysFor',), ('giveawaysFor',), ('hitsFor',), ('xGoalsPercentage', 'corsiPercentage'), ('xGoalsPercentage', 'shotAttemptsFor'), ('xGoalsPercentage', 'goalsFor'), ('xGoalsPercentage', 'faceOffsWonFor'), ('xGoalsPercentage', 'takeawaysFor'), ('xGoalsPercentage', 'giveawaysFor'), ('xGoalsPercentage', 'hitsFor'), ('corsiPercentage', 'shotAttemptsFor'), ('corsiPercentage', 'goalsFor'), ('corsiPercentage', 'faceOffsWonFor'), ('corsiPercentage', 'takeawaysFor'), ('corsiPercentage', 'giveawaysFor'), ('corsiPercentage', 'hitsFor'), ('shotAttemptsFor', 'goalsFor'), ('shotAttemptsFor', 'faceOffsWonFor'), ('shotAttemptsFor', 'takeawaysFor'), ('shotAttemptsFor', 'giveawaysFor'), ('shotAttemptsFor', 'hitsFor'), ('goalsFor', 'faceOffsWonFor'), ('goalsFor', 'takeawaysFor'), ('goalsFor', 'giveawaysFor'), ('goalsFor', 'hitsFor'), ('faceOffsWonFor', 'takeawaysFor'), ('faceOffsWonFor', 'giveawaysFor'), ('faceOffsWonFor', 'hitsFor'), ('takeawaysFor', 'giveawaysFor'), ('takeawaysFor', 'hitsFor'), ('giveawaysFor', 'hitsFor'), ('xGoalsPercentage', 'corsiPercentage', 'shotAttemptsFor'), ('xGoalsPercentage', 'corsiPercentage', 'goalsFor'), ('xGoalsPercentage', 'corsiPercentage', 'faceOffsWonFor'), ('xGoalsPercentage', 'corsiPercentage', 'takeawaysFor'), ('xGoalsPercentage', 'corsiPercentage', 'giveawaysFor'), ('xGoalsPercentage', 'corsiPercentage', 'hitsFor'), ('xGoalsPercentage', 'shotAttemptsFor', 'goalsFor'), ('xGoalsPercentage', 'shotAttemptsFor', 'faceOffsWonFor'), ('xGoalsPercentage', 'shotAttemptsFor', 'takeawaysFor'), ('xGoalsPercentage', 'shotAttemptsFor', 'giveawaysFor'), ('xGoalsPercentage', 'shotAttemptsFor', 'hitsFor'), ('xGoalsPercentage', 'goalsFor', 'faceOffsWonFor'), ('xGoalsPercentage', 'goalsFor', 'takeawaysFor'), ('xGoalsPercentage', 'goalsFor', 'giveawaysFor'), ('xGoalsPercentage', 'goalsFor', 'hitsFor'), ('xGoalsPercentage', 'faceOffsWonFor', 'takeawaysFor'), ('xGoalsPercentage', 'faceOffsWonFor', 'giveawaysFor'), ('xGoalsPercentage', 'faceOffsWonFor', 'hitsFor'), ('xGoalsPercentage', 'takeawaysFor', 'giveawaysFor'), ('xGoalsPercentage', 'takeawaysFor', 'hitsFor'), ('xGoalsPercentage', 'giveawaysFor', 'hitsFor'), ('corsiPercentage', 'shotAttemptsFor', 'goalsFor'), ('corsiPercentage', 'shotAttemptsFor', 'faceOffsWonFor'), ('corsiPercentage', 'shotAttemptsFor', 'takeawaysFor'), ('corsiPercentage', 'shotAttemptsFor', 'giveawaysFor'), ('corsiPercentage', 'shotAttemptsFor', 'hitsFor'), ('corsiPercentage', 'goalsFor', 'faceOffsWonFor'), ('corsiPercentage', 'goalsFor', 'takeawaysFor'), ('corsiPercentage', 'goalsFor', 'giveawaysFor'), ('corsiPercentage', 'goalsFor', 'hitsFor'), ('corsiPercentage', 'faceOffsWonFor', 'takeawaysFor'), ('corsiPercentage', 'faceOffsWonFor', 'giveawaysFor'), ('corsiPercentage', 'faceOffsWonFor', 'hitsFor'), ('corsiPercentage', 'takeawaysFor', 'giveawaysFor'), ('corsiPercentage', 'takeawaysFor', 'hitsFor'), ('corsiPercentage', 'giveawaysFor', 'hitsFor'), ('shotAttemptsFor', 'goalsFor', 'faceOffsWonFor'), ('shotAttemptsFor', 'goalsFor', 'takeawaysFor'), ('shotAttemptsFor', 'goalsFor', 'giveawaysFor'), ('shotAttemptsFor', 'goalsFor', 'hitsFor'), ('shotAttemptsFor', 'faceOffsWonFor', 'takeawaysFor'), ('shotAttemptsFor', 'faceOffsWonFor', 'giveawaysFor'), ('shotAttemptsFor', 'faceOffsWonFor', 'hitsFor'), ('shotAttemptsFor', 'takeawaysFor', 'giveawaysFor'), ('shotAttemptsFor', 'takeawaysFor', 'hitsFor'), ('shotAttemptsFor', 'giveawaysFor', 'hitsFor'), ('goalsFor', 'faceOffsWonFor', 'takeawaysFor'), ('goalsFor', 'faceOffsWonFor', 'giveawaysFor'), ('goalsFor', 'faceOffsWonFor', 'hitsFor'), ('goalsFor', 'takeawaysFor', 'giveawaysFor'), ('goalsFor', 'takeawaysFor', 'hitsFor'), ('goalsFor', 'giveawaysFor', 'hitsFor'), ('faceOffsWonFor', 'takeawaysFor', 'giveawaysFor'), ('faceOffsWonFor', 'takeawaysFor', 'hitsFor'), ('faceOffsWonFor', 'giveawaysFor', 'hitsFor'), ('takeawaysFor', 'giveawaysFor', 'hitsFor'), ('xGoalsPercentage', 'corsiPercentage', 'shotAttemptsFor', 'goalsFor'), ('xGoalsPercentage', 'corsiPercentage', 'shotAttemptsFor', 'faceOffsWonFor'), ('xGoalsPercentage', 'corsiPercentage', 'shotAttemptsFor', 'takeawaysFor'), ('xGoalsPercentage', 'corsiPercentage', 'shotAttemptsFor', 'giveawaysFor'), ('xGoalsPercentage', 'corsiPercentage', 'shotAttemptsFor', 'hitsFor'), ('xGoalsPercentage', 'corsiPercentage', 'goalsFor', 'faceOffsWonFor'), ('xGoalsPercentage', 'corsiPercentage', 'goalsFor', 'takeawaysFor'), ('xGoalsPercentage', 'corsiPercentage', 'goalsFor', 'giveawaysFor'), ('xGoalsPercentage', 'corsiPercentage', 'goalsFor', 'hitsFor'), ('xGoalsPercentage', 'corsiPercentage', 'faceOffsWonFor', 'takeawaysFor'), ('xGoalsPercentage', 'corsiPercentage', 'faceOffsWonFor', 'giveawaysFor'), ('xGoalsPercentage', 'corsiPercentage', 'faceOffsWonFor', 'hitsFor'), ('xGoalsPercentage', 'corsiPercentage', 'takeawaysFor', 'giveawaysFor'), ('xGoalsPercentage', 'corsiPercentage', 'takeawaysFor', 'hitsFor'), ('xGoalsPercentage', 'corsiPercentage', 'giveawaysFor', 'hitsFor'), ('xGoalsPercentage', 'shotAttemptsFor', 'goalsFor', 'faceOffsWonFor'), ('xGoalsPercentage', 'shotAttemptsFor', 'goalsFor', 'takeawaysFor'), ('xGoalsPercentage', 'shotAttemptsFor', 'goalsFor', 'giveawaysFor'), ('xGoalsPercentage', 'shotAttemptsFor', 'goalsFor', 'hitsFor'), ('xGoalsPercentage', 'shotAttemptsFor', 'faceOffsWonFor', 'takeawaysFor'), ('xGoalsPercentage', 'shotAttemptsFor', 'faceOffsWonFor', 'giveawaysFor'), ('xGoalsPercentage', 'shotAttemptsFor', 'faceOffsWonFor', 'hitsFor'), ('xGoalsPercentage', 'shotAttemptsFor', 'takeawaysFor', 'giveawaysFor'), ('xGoalsPercentage', 'shotAttemptsFor', 'takeawaysFor', 'hitsFor'), ('xGoalsPercentage', 'shotAttemptsFor', 'giveawaysFor', 'hitsFor'), ('xGoalsPercentage', 'goalsFor', 'faceOffsWonFor', 'takeawaysFor'), ('xGoalsPercentage', 'goalsFor', 'faceOffsWonFor', 'giveawaysFor'), ('xGoalsPercentage', 'goalsFor', 'faceOffsWonFor', 'hitsFor'), ('xGoalsPercentage', 'goalsFor', 'takeawaysFor', 'giveawaysFor'), ('xGoalsPercentage', 'goalsFor', 'takeawaysFor', 'hitsFor'), ('xGoalsPercentage', 'goalsFor', 'giveawaysFor', 'hitsFor'), ('xGoalsPercentage', 'faceOffsWonFor', 'takeawaysFor', 'giveawaysFor'), ('xGoalsPercentage', 'faceOffsWonFor', 'takeawaysFor', 'hitsFor'), ('xGoalsPercentage', 'faceOffsWonFor', 'giveawaysFor', 'hitsFor'), ('xGoalsPercentage', 'takeawaysFor', 'giveawaysFor', 'hitsFor'), ('corsiPercentage', 'shotAttemptsFor', 'goalsFor', 'faceOffsWonFor'), ('corsiPercentage', 'shotAttemptsFor', 'goalsFor', 'takeawaysFor'), ('corsiPercentage', 'shotAttemptsFor', 'goalsFor', 'giveawaysFor'), ('corsiPercentage', 'shotAttemptsFor', 'goalsFor', 'hitsFor'), ('corsiPercentage', 'shotAttemptsFor', 'faceOffsWonFor', 'takeawaysFor'), ('corsiPercentage', 'shotAttemptsFor', 'faceOffsWonFor', 'giveawaysFor'), ('corsiPercentage', 'shotAttemptsFor', 'faceOffsWonFor', 'hitsFor'), ('corsiPercentage', 'shotAttemptsFor', 'takeawaysFor', 'giveawaysFor'), ('corsiPercentage', 'shotAttemptsFor', 'takeawaysFor', 'hitsFor'), ('corsiPercentage', 'shotAttemptsFor', 'giveawaysFor', 'hitsFor'), ('corsiPercentage', 'goalsFor', 'faceOffsWonFor', 'takeawaysFor'), ('corsiPercentage', 'goalsFor', 'faceOffsWonFor', 'giveawaysFor'), ('corsiPercentage', 'goalsFor', 'faceOffsWonFor', 'hitsFor'), ('corsiPercentage', 'goalsFor', 'takeawaysFor', 'giveawaysFor'), ('corsiPercentage', 'goalsFor', 'takeawaysFor', 'hitsFor'), ('corsiPercentage', 'goalsFor', 'giveawaysFor', 'hitsFor'), ('corsiPercentage', 'faceOffsWonFor', 'takeawaysFor', 'giveawaysFor'), ('corsiPercentage', 'faceOffsWonFor', 'takeawaysFor', 'hitsFor'), ('corsiPercentage', 'faceOffsWonFor', 'giveawaysFor', 'hitsFor'), ('corsiPercentage', 'takeawaysFor', 'giveawaysFor', 'hitsFor'), ('shotAttemptsFor', 'goalsFor', 'faceOffsWonFor', 'takeawaysFor'), ('shotAttemptsFor', 'goalsFor', 'faceOffsWonFor', 'giveawaysFor'), ('shotAttemptsFor', 'goalsFor', 'faceOffsWonFor', 'hitsFor'), ('shotAttemptsFor', 'goalsFor', 'takeawaysFor', 'giveawaysFor'), ('shotAttemptsFor', 'goalsFor', 'takeawaysFor', 'hitsFor'), ('shotAttemptsFor', 'goalsFor', 'giveawaysFor', 'hitsFor'), ('shotAttemptsFor', 'faceOffsWonFor', 'takeawaysFor', 'giveawaysFor'), ('shotAttemptsFor', 'faceOffsWonFor', 'takeawaysFor', 'hitsFor'), ('shotAttemptsFor', 'faceOffsWonFor', 'giveawaysFor', 'hitsFor'), ('shotAttemptsFor', 'takeawaysFor', 'giveawaysFor', 'hitsFor'), ('goalsFor', 'faceOffsWonFor', 'takeawaysFor', 'giveawaysFor'), ('goalsFor', 'faceOffsWonFor', 'takeawaysFor', 'hitsFor'), ('goalsFor', 'faceOffsWonFor', 'giveawaysFor', 'hitsFor'), ('goalsFor', 'takeawaysFor', 'giveawaysFor', 'hitsFor'), ('faceOffsWonFor', 'takeawaysFor', 'giveawaysFor', 'hitsFor'), ('xGoalsPercentage', 'corsiPercentage', 'shotAttemptsFor', 'goalsFor', 'faceOffsWonFor'), ('xGoalsPercentage', 'corsiPercentage', 'shotAttemptsFor', 'goalsFor', 'takeawaysFor'), ('xGoalsPercentage', 'corsiPercentage', 'shotAttemptsFor', 'goalsFor', 'giveawaysFor'), ('xGoalsPercentage', 'corsiPercentage', 'shotAttemptsFor', 'goalsFor', 'hitsFor'), ('xGoalsPercentage', 'corsiPercentage', 'shotAttemptsFor', 'faceOffsWonFor', 'takeawaysFor'), ('xGoalsPercentage', 'corsiPercentage', 'shotAttemptsFor', 'faceOffsWonFor', 'giveawaysFor'), ('xGoalsPercentage', 'corsiPercentage', 'shotAttemptsFor', 'faceOffsWonFor', 'hitsFor'), ('xGoalsPercentage', 'corsiPercentage', 'shotAttemptsFor', 'takeawaysFor', 'giveawaysFor'), ('xGoalsPercentage', 'corsiPercentage', 'shotAttemptsFor', 'takeawaysFor', 'hitsFor'), ('xGoalsPercentage', 'corsiPercentage', 'shotAttemptsFor', 'giveawaysFor', 'hitsFor'), ('xGoalsPercentage', 'corsiPercentage', 'goalsFor', 'faceOffsWonFor', 'takeawaysFor'), ('xGoalsPercentage', 'corsiPercentage', 'goalsFor', 'faceOffsWonFor', 'giveawaysFor'), ('xGoalsPercentage', 'corsiPercentage', 'goalsFor', 'faceOffsWonFor', 'hitsFor'), ('xGoalsPercentage', 'corsiPercentage', 'goalsFor', 'takeawaysFor', 'giveawaysFor'), ('xGoalsPercentage', 'corsiPercentage', 'goalsFor', 'takeawaysFor', 'hitsFor'), ('xGoalsPercentage', 'corsiPercentage', 'goalsFor', 'giveawaysFor', 'hitsFor'), ('xGoalsPercentage', 'corsiPercentage', 'faceOffsWonFor', 'takeawaysFor', 'giveawaysFor'), ('xGoalsPercentage', 'corsiPercentage', 'faceOffsWonFor', 'takeawaysFor', 'hitsFor'), ('xGoalsPercentage', 'corsiPercentage', 'faceOffsWonFor', 'giveawaysFor', 'hitsFor'), ('xGoalsPercentage', 'corsiPercentage', 'takeawaysFor', 'giveawaysFor', 'hitsFor'), ('xGoalsPercentage', 'shotAttemptsFor', 'goalsFor', 'faceOffsWonFor', 'takeawaysFor'), ('xGoalsPercentage', 'shotAttemptsFor', 'goalsFor', 'faceOffsWonFor', 'giveawaysFor'), ('xGoalsPercentage', 'shotAttemptsFor', 'goalsFor', 'faceOffsWonFor', 'hitsFor'), ('xGoalsPercentage', 'shotAttemptsFor', 'goalsFor', 'takeawaysFor', 'giveawaysFor'), ('xGoalsPercentage', 'shotAttemptsFor', 'goalsFor', 'takeawaysFor', 'hitsFor'), ('xGoalsPercentage', 'shotAttemptsFor', 'goalsFor', 'giveawaysFor', 'hitsFor'), ('xGoalsPercentage', 'shotAttemptsFor', 'faceOffsWonFor', 'takeawaysFor', 'giveawaysFor'), ('xGoalsPercentage', 'shotAttemptsFor', 'faceOffsWonFor', 'takeawaysFor', 'hitsFor'), ('xGoalsPercentage', 'shotAttemptsFor', 'faceOffsWonFor', 'giveawaysFor', 'hitsFor'), ('xGoalsPercentage', 'shotAttemptsFor', 'takeawaysFor', 'giveawaysFor', 'hitsFor'), ('xGoalsPercentage', 'goalsFor', 'faceOffsWonFor', 'takeawaysFor', 'giveawaysFor'), ('xGoalsPercentage', 'goalsFor', 'faceOffsWonFor', 'takeawaysFor', 'hitsFor'), ('xGoalsPercentage', 'goalsFor', 'faceOffsWonFor', 'giveawaysFor', 'hitsFor'), ('xGoalsPercentage', 'goalsFor', 'takeawaysFor', 'giveawaysFor', 'hitsFor'), ('xGoalsPercentage', 'faceOffsWonFor', 'takeawaysFor', 'giveawaysFor', 'hitsFor'), ('corsiPercentage', 'shotAttemptsFor', 'goalsFor', 'faceOffsWonFor', 'takeawaysFor'), ('corsiPercentage', 'shotAttemptsFor', 'goalsFor', 'faceOffsWonFor', 'giveawaysFor'), ('corsiPercentage', 'shotAttemptsFor', 'goalsFor', 'faceOffsWonFor', 'hitsFor'), ('corsiPercentage', 'shotAttemptsFor', 'goalsFor', 'takeawaysFor', 'giveawaysFor'), ('corsiPercentage', 'shotAttemptsFor', 'goalsFor', 'takeawaysFor', 'hitsFor'), ('corsiPercentage', 'shotAttemptsFor', 'goalsFor', 'giveawaysFor', 'hitsFor'), ('corsiPercentage', 'shotAttemptsFor', 'faceOffsWonFor', 'takeawaysFor', 'giveawaysFor'), ('corsiPercentage', 'shotAttemptsFor', 'faceOffsWonFor', 'takeawaysFor', 'hitsFor'), ('corsiPercentage', 'shotAttemptsFor', 'faceOffsWonFor', 'giveawaysFor', 'hitsFor'), ('corsiPercentage', 'shotAttemptsFor', 'takeawaysFor', 'giveawaysFor', 'hitsFor'), ('corsiPercentage', 'goalsFor', 'faceOffsWonFor', 'takeawaysFor', 'giveawaysFor'), ('corsiPercentage', 'goalsFor', 'faceOffsWonFor', 'takeawaysFor', 'hitsFor'), ('corsiPercentage', 'goalsFor', 'faceOffsWonFor', 'giveawaysFor', 'hitsFor'), ('corsiPercentage', 'goalsFor', 'takeawaysFor', 'giveawaysFor', 'hitsFor'), ('corsiPercentage', 'faceOffsWonFor', 'takeawaysFor', 'giveawaysFor', 'hitsFor'), ('shotAttemptsFor', 'goalsFor', 'faceOffsWonFor', 'takeawaysFor', 'giveawaysFor'), ('shotAttemptsFor', 'goalsFor', 'faceOffsWonFor', 'takeawaysFor', 'hitsFor'), ('shotAttemptsFor', 'goalsFor', 'faceOffsWonFor', 'giveawaysFor', 'hitsFor'), ('shotAttemptsFor', 'goalsFor', 'takeawaysFor', 'giveawaysFor', 'hitsFor'), ('shotAttemptsFor', 'faceOffsWonFor', 'takeawaysFor', 'giveawaysFor', 'hitsFor'), ('goalsFor', 'faceOffsWonFor', 'takeawaysFor', 'giveawaysFor', 'hitsFor'), ('xGoalsPercentage', 'corsiPercentage', 'shotAttemptsFor', 'goalsFor', 'faceOffsWonFor', 'takeawaysFor'), ('xGoalsPercentage', 'corsiPercentage', 'shotAttemptsFor', 'goalsFor', 'faceOffsWonFor', 'giveawaysFor'), ('xGoalsPercentage', 'corsiPercentage', 'shotAttemptsFor', 'goalsFor', 'faceOffsWonFor', 'hitsFor'), ('xGoalsPercentage', 'corsiPercentage', 'shotAttemptsFor', 'goalsFor', 'takeawaysFor', 'giveawaysFor'), ('xGoalsPercentage', 'corsiPercentage', 'shotAttemptsFor', 'goalsFor', 'takeawaysFor', 'hitsFor'), ('xGoalsPercentage', 'corsiPercentage', 'shotAttemptsFor', 'goalsFor', 'giveawaysFor', 'hitsFor'), ('xGoalsPercentage', 'corsiPercentage', 'shotAttemptsFor', 'faceOffsWonFor', 'takeawaysFor', 'giveawaysFor'), ('xGoalsPercentage', 'corsiPercentage', 'shotAttemptsFor', 'faceOffsWonFor', 'takeawaysFor', 'hitsFor'), ('xGoalsPercentage', 'corsiPercentage', 'shotAttemptsFor', 'faceOffsWonFor', 'giveawaysFor', 'hitsFor'), ('xGoalsPercentage', 'corsiPercentage', 'shotAttemptsFor', 'takeawaysFor', 'giveawaysFor', 'hitsFor'), ('xGoalsPercentage', 'corsiPercentage', 'goalsFor', 'faceOffsWonFor', 'takeawaysFor', 'giveawaysFor'), ('xGoalsPercentage', 'corsiPercentage', 'goalsFor', 'faceOffsWonFor', 'takeawaysFor', 'hitsFor'), ('xGoalsPercentage', 'corsiPercentage', 'goalsFor', 'faceOffsWonFor', 'giveawaysFor', 'hitsFor'), ('xGoalsPercentage', 'corsiPercentage', 'goalsFor', 'takeawaysFor', 'giveawaysFor', 'hitsFor'), ('xGoalsPercentage', 'corsiPercentage', 'faceOffsWonFor', 'takeawaysFor', 'giveawaysFor', 'hitsFor'), ('xGoalsPercentage', 'shotAttemptsFor', 'goalsFor', 'faceOffsWonFor', 'takeawaysFor', 'giveawaysFor'), ('xGoalsPercentage', 'shotAttemptsFor', 'goalsFor', 'faceOffsWonFor', 'takeawaysFor', 'hitsFor'), ('xGoalsPercentage', 'shotAttemptsFor', 'goalsFor', 'faceOffsWonFor', 'giveawaysFor', 'hitsFor'), ('xGoalsPercentage', 'shotAttemptsFor', 'goalsFor', 'takeawaysFor', 'giveawaysFor', 'hitsFor'), ('xGoalsPercentage', 'shotAttemptsFor', 'faceOffsWonFor', 'takeawaysFor', 'giveawaysFor', 'hitsFor'), ('xGoalsPercentage', 'goalsFor', 'faceOffsWonFor', 'takeawaysFor', 'giveawaysFor', 'hitsFor'), ('corsiPercentage', 'shotAttemptsFor', 'goalsFor', 'faceOffsWonFor', 'takeawaysFor', 'giveawaysFor'), ('corsiPercentage', 'shotAttemptsFor', 'goalsFor', 'faceOffsWonFor', 'takeawaysFor', 'hitsFor'), ('corsiPercentage', 'shotAttemptsFor', 'goalsFor', 'faceOffsWonFor', 'giveawaysFor', 'hitsFor'), ('corsiPercentage', 'shotAttemptsFor', 'goalsFor', 'takeawaysFor', 'giveawaysFor', 'hitsFor'), ('corsiPercentage', 'shotAttemptsFor', 'faceOffsWonFor', 'takeawaysFor', 'giveawaysFor', 'hitsFor'), ('corsiPercentage', 'goalsFor', 'faceOffsWonFor', 'takeawaysFor', 'giveawaysFor', 'hitsFor'), ('shotAttemptsFor', 'goalsFor', 'faceOffsWonFor', 'takeawaysFor', 'giveawaysFor', 'hitsFor'), ('xGoalsPercentage', 'corsiPercentage', 'shotAttemptsFor', 'goalsFor', 'faceOffsWonFor', 'takeawaysFor', 'giveawaysFor'), ('xGoalsPercentage', 'corsiPercentage', 'shotAttemptsFor', 'goalsFor', 'faceOffsWonFor', 'takeawaysFor', 'hitsFor'), ('xGoalsPercentage', 'corsiPercentage', 'shotAttemptsFor', 'goalsFor', 'faceOffsWonFor', 'giveawaysFor', 'hitsFor'), ('xGoalsPercentage', 'corsiPercentage', 'shotAttemptsFor', 'goalsFor', 'takeawaysFor', 'giveawaysFor', 'hitsFor'), ('xGoalsPercentage', 'corsiPercentage', 'shotAttemptsFor', 'faceOffsWonFor', 'takeawaysFor', 'giveawaysFor', 'hitsFor'), ('xGoalsPercentage', 'corsiPercentage', 'goalsFor', 'faceOffsWonFor', 'takeawaysFor', 'giveawaysFor', 'hitsFor'), ('xGoalsPercentage', 'shotAttemptsFor', 'goalsFor', 'faceOffsWonFor', 'takeawaysFor', 'giveawaysFor', 'hitsFor'), ('corsiPercentage', 'shotAttemptsFor', 'goalsFor', 'faceOffsWonFor', 'takeawaysFor', 'giveawaysFor', 'hitsFor'), ('xGoalsPercentage', 'corsiPercentage', 'shotAttemptsFor', 'goalsFor', 'faceOffsWonFor', 'takeawaysFor', 'giveawaysFor', 'hitsFor')]\n\n\n\nX_train_df=pd.DataFrame(X_train, columns=features)\nX_test_df=pd.DataFrame(X_test, columns=features)\n\nimport scipy.stats\nfrom scipy.stats import spearmanr\nimport itertools\n\n#calculate spearman correlation coefficients for each subset\ndef mean_xx_corr(x_df):\n    df_colnames=x_df.columns\n    xx_corrs=[]\n\n    df_colname_pairs=itertools.combinations(df_colnames, 2)\n    for colname1, colname2 in df_colname_pairs:\n        col1=x_df[colname1]\n        col2=x_df[colname2]\n        xx_pair_corr=scipy.stats.spearmanr(col1, col2)\n        xx_corrs.append(xx_pair_corr)\n    \n    return np.mean(xx_corrs)\n\n\ndef compute_mean_xy_corr(x_df, y_vec):\n    df_colnames=x_df.columns\n    xy_corrs=[]\n    for colname in df_colnames:\n        x_col = x_df[colname]\n        xy_pair_corr = spearmanr(x_col, y_vec)\n        xy_corrs.append(xy_pair_corr)\n\n    return np.mean(xy_corrs)\n\n\n0.22997578303855906 0.21755500905626496\n\n\n\nbest_subset=np.NaN\nbest_merit_score=0\n\n\nfor i in range(len(feature_subset)):\n    subset=feature_subset[i]\n    if len(subset)==1:\n        df_sub=df[[subset[0]]]\n    if len(subset)==2:\n        df_sub=df[[subset[0],subset[1]]]\n    if len(subset)==3:\n        df_sub=df[[subset[0],subset[1],subset[2]]]\n    if len(subset)==4:\n        df_sub=df[[subset[0],subset[1],subset[2],subset[3]]]\n    if len(subset)==5:\n        df_sub=df[[subset[0],subset[1],subset[2],subset[3],subset[4]]]\n    if len(subset)==6:\n        df_sub=df[[subset[0],subset[1],subset[2],subset[3],subset[4],subset[5]]]\n    if len(subset)==7:\n        df_sub=df[[subset[0],subset[1],subset[2],subset[3],subset[4],subset[5],subset[6]]]\n    if len(subset)==8:\n        df_sub=df[[subset[0],subset[1],subset[2],subset[3],subset[4],subset[5],subset[6],subset[7]]]\n\n    if len(subset)&gt;0:\n        xx_corr=mean_xx_corr(df_sub)\n        xy_corr=compute_mean_xy_corr(df_sub, label)\n        k=len(subset)\n        merit_score_numer = k * np.absolute(xy_corr)\n        merit_score_denom = np.sqrt(k + k * (k + 1) * np.absolute(xx_corr))\n        merit_score_s2 = merit_score_numer / merit_score_denom\n\n    if merit_score_s2 &gt; best_merit_score:\n        best_merit_score=merit_score_s2\n        best_subset=subset\n\n\nprint(\"The best subset is: \",best_subset,\"which has a merit score of\",best_merit_score)\n\n\n    \n\n\n            \n\n\n\n\n\n    \n    \n   \n\n/Users/gracehyland/anaconda3/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n  return _methods._mean(a, axis=axis, dtype=dtype,\n/Users/gracehyland/anaconda3/lib/python3.10/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n  ret = ret.dtype.type(ret / rcount)\n/Users/gracehyland/anaconda3/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n  return _methods._mean(a, axis=axis, dtype=dtype,\n/Users/gracehyland/anaconda3/lib/python3.10/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n  ret = ret.dtype.type(ret / rcount)\n/Users/gracehyland/anaconda3/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n  return _methods._mean(a, axis=axis, dtype=dtype,\n/Users/gracehyland/anaconda3/lib/python3.10/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n  ret = ret.dtype.type(ret / rcount)\n/Users/gracehyland/anaconda3/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n  return _methods._mean(a, axis=axis, dtype=dtype,\n/Users/gracehyland/anaconda3/lib/python3.10/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n  ret = ret.dtype.type(ret / rcount)\n/Users/gracehyland/anaconda3/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n  return _methods._mean(a, axis=axis, dtype=dtype,\n/Users/gracehyland/anaconda3/lib/python3.10/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n  ret = ret.dtype.type(ret / rcount)\n/Users/gracehyland/anaconda3/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n  return _methods._mean(a, axis=axis, dtype=dtype,\n/Users/gracehyland/anaconda3/lib/python3.10/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n  ret = ret.dtype.type(ret / rcount)\n/Users/gracehyland/anaconda3/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n  return _methods._mean(a, axis=axis, dtype=dtype,\n/Users/gracehyland/anaconda3/lib/python3.10/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n  ret = ret.dtype.type(ret / rcount)\n/Users/gracehyland/anaconda3/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n  return _methods._mean(a, axis=axis, dtype=dtype,\n/Users/gracehyland/anaconda3/lib/python3.10/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n  ret = ret.dtype.type(ret / rcount)\n\n\nThe best subset is:  ('corsiPercentage', 'giveawaysFor', 'hitsFor') which has a merit score of 0.4721158842275389\n\n\n\n\nNaive Bayes\nThe first NB model I used incorporated the data from all seven features. The GaussianNB model from the sklearn package was trained on the previously partitioned train data.\n\nfrom sklearn.naive_bayes import GaussianNB\n\nmodel=GaussianNB()\n\nmodel.fit(X_train, y_train)\n\nfrom sklearn.metrics import accuracy_score \nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\n\ny_pred=model.predict(X_test)\naccuracy=accuracy_score(y_pred, y_test)\nf1 = f1_score(y_pred, y_test, average=\"weighted\")\n\nprint(accuracy)\nprint(f1)\n\ncm=confusion_matrix(y_test, y_pred)\ndisp=ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Non-playoff team\",\"Playoff team\"])\ndisp.plot()\n\n\n0.8076923076923077\n0.809839283523494\n\n\n&lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7fec80b0ece0&gt;\n\n\n\n\n\nAfter training on the training data, I had the model predict whether the records in the test data were playoff teams based on their features. The accuracy and f1 scores were both around 80% which is pretty high and the confusion matrix shows that the model was equally proficient at predicting teams to make the playoffs as they are at predicting teams to miss the playoffs.\nFurther, I would like to test other subsets of features to find a more accurate model. And then carry out the same process to evaluate models based on regular 5 on 5 play and compare the accuracy of these two models.\nFor now, it is clear to see that player evaluation metrics during power plays can be used to predict regular season success.\n\n\nX = feature_matrix[[\"corsiPercentage\",\"hitsFor\",\"giveawaysFor\"]]\ny= label\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size=0.2,\n    random_state=100\n)\n\nmodel2=GaussianNB()\n\nmodel2.fit(X_train, y_train)\n\ny_pred=model2.predict(X_test)\naccuracy=accuracy_score(y_pred, y_test)\nf1 = f1_score(y_pred, y_test, average=\"weighted\")\n\nprint(accuracy)\nprint(f1)\n\ncm=confusion_matrix(y_test, y_pred)\ndisp=ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Non-playoff team\",\"Playoff team\"])\ndisp.plot()\n\n0.7307692307692307\n0.7287014061207611\n\n\n&lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7feca3008d30&gt;"
  },
  {
    "objectID": "naive_bayes.html#naive-bayes-model-using-powerplay-data",
    "href": "naive_bayes.html#naive-bayes-model-using-powerplay-data",
    "title": "Introduction to Naive Bayes",
    "section": "Naive Bayes Model Using Powerplay Data",
    "text": "Naive Bayes Model Using Powerplay Data\n\nPrepare Power Play Data for Naive Bayes\nI prepared the team powerplay data for Naive Bayes classification in an R file linked in the Code page. Team data from the 2018-2019, 2020-2021, 2021-2022, and 2022-2023 seasons were combined and the desired features were subsetted from the greater dataset. An additional variable was added for whether or not the team made the playoffs that season and all numerical metrics were normalized. Finally, all non-numerical variables except the playoff variable were removed. To begin building the model on powerplay data, I first had to import the necessary libraries.\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport sklearn\nfrom sklearn import metrics\nimport scipy.stats\nfrom scipy.stats import spearmanr\nimport itertools\nfrom itertools import chain, combinations\n\n\nI loaded the previously cleaned dataset “team_pp_data_clean.csv” and prepared it for use by a Naive Bayes model by creating a separate dataframe to hold the labels and then removed the column of labels to create a data frame that holds the features. The values of the features were already normalized in the cleaning process. I also created a variable holding a list of the feature names.\n\n\nCode\ndf=pd.read_csv(\"../data/01-modified-data/team_pp_data_clean.csv\")\n\nlabel = df['playoff'].copy()\ndf=df.drop(columns=['Unnamed: 0','playoff'])\nfeatures=df.columns\nfeature_matrix = df[features].copy()\n\n\nAfter loading the pre-prepared dataset, I used sklearn to split the dataset into a training set and a test set with an 80-20 split. It is important to keep a portion of the data aside to test the accuracy of the model on so as to prevent overfitting to the test data.\n\n\nCode\nfrom sklearn.model_selection import train_test_split\n\nX = feature_matrix\ny= label\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size=0.2,\n    random_state=100\n)\n\n\n\n\nNaive Bayes\nThe first NB model I used incorporated the data from all eight features. The GaussianNB model from the sklearn package was trained on the previously partitioned training data.\n\n\nCode\nfrom sklearn.naive_bayes import GaussianNB\n\nmodel=GaussianNB()\n\nmodel.fit(X_train, y_train)\n\nfrom sklearn.metrics import accuracy_score \nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\n\ny_pred=model.predict(X_test)\naccuracy=accuracy_score(y_pred, y_test)\nf1 = f1_score(y_pred, y_test, average=\"weighted\")\n\nprint(\"Accuracy score:\",accuracy)\nprint(\"F1 score:\",f1)\nprint(\"Precision:\", sklearn.metrics.precision_score(y_pred,y_test))\n\ncm=confusion_matrix(y_test, y_pred)\ndisp=ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Non-playoff team\",\"Playoff team\"])\ndisp.plot()\n\n\nAccuracy score: 0.8076923076923077\nF1 score: 0.809839283523494\nPrecision: 0.875\n\n\n&lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7fdf69563220&gt;\n\n\n\n\n\nAfter training on the training data, I had the model predict whether the records in the test data were playoff teams based on their features. The accuracy and f1 scores were both around 80% which is pretty high, the precision score is 87.5% and the confusion matrix shows that the model was equally proficient at predicting teams to make the playoffs as they are at predicting teams to miss the playoffs.\nFurther, I would like to test other subsets of features to find a more accurate model. And then carry out the same process to evaluate models based on regular 5 on 5 play and compare the accuracy of these two models.\nFor now, it is clear to see that player evaluation metrics during power plays can be used to predict regular season success.\n\n\nFeature Selection\nFeature selection is a process by which only the most predictive and least correlated features are used to train a model in order to maximize accuracy.\nTo find the subset of features that result in the highest accuracy score, I will find every possible combination of the eight features and calculate a metric to evaluate their correlation coefficients.\n\n\nCode\n#list all possible feature subsets\nfeature_subsets = list(features)\nfeature_subset=chain.from_iterable(combinations(feature_subsets,r) for r in range(len(feature_subsets)+1))\nfeature_subset=list(feature_subset)\n\n\nAfter creating a list of all possible feature subsets from the data, I created functions to calculate the average correlation between all pairs of features in any given subset and the average correlation between each feature in the subset and the label data. The values returned by the function can be used to calculate a merit score for each feature subset to determine the best feature subset. The best feature subset will be a set of features that are minimally correlated with each other and maximally correlated with the label data.\n\n\nCode\n#calculate spearman correlation coefficients for each subset\n\n#function for mean correlation between each pair of features in the subset\ndef mean_xx_corr(x_df):\n    df_colnames=x_df.columns\n    xx_corrs=[]\n\n    df_colname_pairs=itertools.combinations(df_colnames, 2)\n    for colname1, colname2 in df_colname_pairs:\n        col1=x_df[colname1]\n        col2=x_df[colname2]\n        xx_pair_corr=scipy.stats.spearmanr(col1, col2)\n        xx_corrs.append(xx_pair_corr)\n    \n    return np.mean(xx_corrs)\n\n#function for mean correlation between each feature in the subset and the label\ndef compute_mean_xy_corr(x_df, y_vec):\n    df_colnames=x_df.columns\n    xy_corrs=[]\n    for colname in df_colnames:\n        x_col = x_df[colname]\n        xy_pair_corr = spearmanr(x_col, y_vec)\n        xy_corrs.append(xy_pair_corr)\n\n    return np.mean(xy_corrs)\n\n\nUsing the functions above, I iterated over each possible subset to calculate the merit score for all feature subsets.\n\n\nCode\nbest_subset=np.NaN\nbest_merit_score=0\n\n\nfor i in range(len(feature_subset)):\n    subset=feature_subset[i]\n    if len(subset)==1:\n        df_sub=df[[subset[0]]]\n    if len(subset)==2:\n        df_sub=df[[subset[0],subset[1]]]\n    if len(subset)==3:\n        df_sub=df[[subset[0],subset[1],subset[2]]]\n    if len(subset)==4:\n        df_sub=df[[subset[0],subset[1],subset[2],subset[3]]]\n    if len(subset)==5:\n        df_sub=df[[subset[0],subset[1],subset[2],subset[3],subset[4]]]\n    if len(subset)==6:\n        df_sub=df[[subset[0],subset[1],subset[2],subset[3],subset[4],subset[5]]]\n    if len(subset)==7:\n        df_sub=df[[subset[0],subset[1],subset[2],subset[3],subset[4],subset[5],subset[6]]]\n    if len(subset)==8:\n        df_sub=df[[subset[0],subset[1],subset[2],subset[3],subset[4],subset[5],subset[6],subset[7]]]\n\n    if len(subset)&gt;0:\n        xx_corr=mean_xx_corr(df_sub)\n        xy_corr=compute_mean_xy_corr(df_sub, label)\n        k=len(subset)\n        merit_score_numer = k * np.absolute(xy_corr)\n        merit_score_denom = np.sqrt(k + k * (k + 1) * np.absolute(xx_corr))\n        merit_score_s2 = merit_score_numer / merit_score_denom\n        \n    if len(subset)==0:\n        merit_score_s2=0\n\n    if merit_score_s2 &gt; best_merit_score:\n        best_merit_score=merit_score_s2\n        best_subset=subset\n\n\n\n\nCode\nprint(\"The best subset is: \",best_subset,\"which has a merit score of\",best_merit_score)\n\n\nThe best subset is:  ('corsiPercentage', 'giveawaysFor', 'hitsFor') which has a merit score of 0.4721158842275389\n\n\nThe best subset based on the merit score was a subset with the three features corsiPercentage, giveawaysFor, and hitsFor. Using this subset, I trained a new Naive Bayes model using only these three features.\n\n\nCode\nX = feature_matrix[[\"corsiPercentage\",\"hitsFor\",\"giveawaysFor\"]]\ny= label\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size=0.2,\n    random_state=100\n)\n\nmodel2=GaussianNB()\n\nmodel2.fit(X_train, y_train)\n\ny_pred=model2.predict(X_test)\naccuracy=accuracy_score(y_pred, y_test)\nf1 = f1_score(y_pred, y_test, average=\"weighted\")\n\nprint(\"Accuracy score:\",accuracy)\nprint(\"F1 score:\",f1)\nprint(\"Precision:\", sklearn.metrics.precision_score(y_pred,y_test))\n\ncm=confusion_matrix(y_test, y_pred)\ndisp=ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Non-playoff team\",\"Playoff team\"])\ndisp.plot()\n\n\nAccuracy score: 0.7307692307692307\nF1 score: 0.7287014061207611\nPrecision: 0.75\n\n\n&lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7fdf7a847ac0&gt;\n\n\n\n\n\nUsing the best subset determined from the feature selection process did result in lower accuracy, precision, and f1 score than the model that used all eight features. Based on a comparison of the confusion matrices from the two models, the difference in accuracy appears to come from the model with fewer features assigning an inaccurate Non-playoff team label to two additional true Playoff teams. The difference is very minimal between these two so the computational efficiency of using three features instead of eight means the model with feature selection is likely better because it will also be less prone to overfitting if employed on a larger dataset."
  },
  {
    "objectID": "naive_bayes.html#naive-bayes-model-using-regular-play-data",
    "href": "naive_bayes.html#naive-bayes-model-using-regular-play-data",
    "title": "Introduction to Naive Bayes",
    "section": "Naive Bayes Model Using Regular Play Data",
    "text": "Naive Bayes Model Using Regular Play Data\nAfter evaluating a Naive Bayes model based only on power play data, I want to test my hypothesis that metrics from playoff minutes can be more predictive of regular season success than metrics from regular play minutes by comparing the accuracy of a model based on regular play data.\n\nPrepare Regular Play Data for Naive Bayes\nUsing the cleaned data set “team_regular_data_clean.csv” I followed the same process for preparing the data for the Naive Bayes model as detailed above with the power play data.\n\n\nCode\ndf=pd.read_csv(\"../data/01-modified-data/team_regular_data_clean.csv\")\n\nlabel = df['playoff'].copy()\ndf=df.drop(columns=['Unnamed: 0','playoff'])\nfeatures=df.columns\nfeature_matrix = df[features].copy()\n\nX = feature_matrix\ny= label\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size=0.2,\n    random_state=100\n)\n\n\n\n\nNaive Bayes\nUsing the prepared and split data from above, I trained a Naive Bayes model on regular play data with all eight features and calculated accuracy, precision, and f1 scores and constructed a confusion matrix.\n\n\nCode\nmodel=GaussianNB()\nmodel.fit(X_train, y_train)\n\ny_pred=model.predict(X_test)\naccuracy=accuracy_score(y_pred, y_test)\nf1 = f1_score(y_pred, y_test, average=\"weighted\")\n\nprint(\"Accuracy score:\",accuracy)\nprint(\"F1 score:\",f1)\nprint(\"Precision:\", sklearn.metrics.precision_score(y_pred,y_test))\n\ncm=confusion_matrix(y_test, y_pred)\ndisp=ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Non-playoff team\",\"Playoff team\"])\ndisp.plot()\n\n\nAccuracy score: 0.7307692307692307\nF1 score: 0.7287749287749288\nPrecision: 0.625\n\n\n&lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7fdf893d7010&gt;\n\n\n\n\n\nThe accuracy and precision scores for this model are very similar to the accuracy and precision scores for the model with feature selection on power play data, but the precision score is lower than the precision score for either power play based models.\n\n\nFeature Selection\nFor a full comparison between regular data and power play data, I will now apply the same feature selection process to regular play data and train a model on the ideal subset of regular play data.\n\n\nCode\n#list all possible feature subsets\nfeature_subsets = list(features)\nfeature_subset=chain.from_iterable(combinations(feature_subsets,r) for r in range(len(feature_subsets)+1))\nfeature_subset=list(feature_subset)\n\n\nbest_subset=np.NaN\nbest_merit_score=0\n\n\nfor i in range(len(feature_subset)):\n    subset=feature_subset[i]\n    if len(subset)==1:\n        df_sub=df[[subset[0]]]\n    if len(subset)==2:\n        df_sub=df[[subset[0],subset[1]]]\n    if len(subset)==3:\n        df_sub=df[[subset[0],subset[1],subset[2]]]\n    if len(subset)==4:\n        df_sub=df[[subset[0],subset[1],subset[2],subset[3]]]\n    if len(subset)==5:\n        df_sub=df[[subset[0],subset[1],subset[2],subset[3],subset[4]]]\n    if len(subset)==6:\n        df_sub=df[[subset[0],subset[1],subset[2],subset[3],subset[4],subset[5]]]\n    if len(subset)==7:\n        df_sub=df[[subset[0],subset[1],subset[2],subset[3],subset[4],subset[5],subset[6]]]\n    if len(subset)==8:\n        df_sub=df[[subset[0],subset[1],subset[2],subset[3],subset[4],subset[5],subset[6],subset[7]]]\n\n    if len(subset)&gt;0:\n        xx_corr=mean_xx_corr(df_sub)\n        xy_corr=compute_mean_xy_corr(df_sub, label)\n        k=len(subset)\n        merit_score_numer = k * np.absolute(xy_corr)\n        merit_score_denom = np.sqrt(k + k * (k + 1) * np.absolute(xx_corr))\n        merit_score_s2 = merit_score_numer / merit_score_denom\n\n    if merit_score_s2 &gt; best_merit_score:\n        best_merit_score=merit_score_s2\n        best_subset=subset\n\n\n\n\nCode\nprint(\"The best subset is: \",best_subset,\"which has a merit score of\",best_merit_score)\n\n\nThe best subset is:  ('xGoalsPercentage', 'corsiPercentage', 'goalsFor', 'giveawaysFor', 'hitsFor') which has a merit score of 0.3988440207335123\n\n\nThe best subset features for regular play data is expected goals percentage, Corsi percentage, goals for, giveaways for, and hits for. This includes all the features from the power play subset in addition to expected goals percentage and goalsFor. The increase in features for regular data may indicate that less data is needed from power play play to make an equally accurate prediction on regular season success for a team.\n\n\nNaive Bayes After Feature Selection\nI trained and tested a model on this subset of features and calculated accuracy score, precision score, and f1 score and constructed a confusion matrix for the test set of the model.\n\n\nCode\nX = feature_matrix[[\"xGoalsPercentage\",\"goalsFor\",\"corsiPercentage\",\"hitsFor\",\"giveawaysFor\"]]\ny= label\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size=0.2,\n    random_state=100\n)\n\nmodel2=GaussianNB()\n\nmodel2.fit(X_train, y_train)\n\ny_pred=model2.predict(X_test)\naccuracy=accuracy_score(y_pred, y_test)\nf1 = f1_score(y_pred, y_test, average=\"weighted\")\n\nprint(\"Accuracy score:\",accuracy)\nprint(\"F1 score:\",f1)\nprint(\"Precision:\", sklearn.metrics.precision_score(y_pred,y_test))\n\ncm=confusion_matrix(y_test, y_pred)\ndisp=ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Non-playoff team\",\"Playoff team\"])\ndisp.plot()\n\n\nAccuracy score: 0.7692307692307693\nF1 score: 0.7692307692307693\nPrecision: 0.625\n\n\n&lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7fdf78bc8f70&gt;\n\n\n\n\n\nThe regular play model with feature selection has slightly higher accuracy and f1 scores than the power play model with feature selection, but not higher than the power play model with all features used. Howevery, the precision score is quite a bit lower than both power play data models. This model seems to be good at classifying non-playoff teams, but has a lower success rate with identifying teams that do qualify for the playoffs."
  },
  {
    "objectID": "dimensionality-reduction.html#dimensionality-reduction-with-pca",
    "href": "dimensionality-reduction.html#dimensionality-reduction-with-pca",
    "title": "Dimensionality Reduction",
    "section": "Dimensionality Reduction with PCA",
    "text": "Dimensionality Reduction with PCA\nTo start, I reduced the dimensions using PCA. I first had to import the necessary libraries and load in the data set.\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\ndf=pd.read_csv(\"../data/01-modified-data/team_pp_data_clean.csv\")\nlabel = df['playoff'].copy()\ndf=df.drop(columns=['Unnamed: 0','playoff'])\n\n\nI initially carried out PCA with reduction to three components and applied it to the data frame. I added the newly created variables to a data frame with column names and printed the head of the data frame to visualize.\n\n\nCode\npca=PCA(n_components=3)\npca.fit(df)\nprint(pca.components_)\ndata_pca=pca.transform(df)\ndata_pca=pd.DataFrame(data_pca,columns=['A','B','C'])\ndata_pca.head()\n\n\n[[ 0.01540233  0.00178136  0.2858614   0.33675383  0.23802378  0.67930777\n   0.39066758  0.36592922]\n [ 0.00996335  0.00628949  0.28774976  0.56100297  0.35014151 -0.64675638\n   0.24591146 -0.03117027]\n [-0.04444102 -0.02736794 -0.0379361  -0.30857632  0.04736612 -0.31303529\n  -0.02653487  0.89424759]]\n\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\n0.866603\n-0.243610\n0.167068\n\n\n1\n1.115348\n0.319936\n-0.316053\n\n\n2\n0.066727\n-0.098512\n-0.101846\n\n\n3\n-0.373166\n0.101799\n-0.174254\n\n\n4\n1.504929\n-0.260650\n0.340209\n\n\n\n\n\n\n\nAfter performing PCA with three components, I created a graph of cumulative proportion of variance explained against the number of components ranging from 1 to 8 to see the lowest number of components that can still explain most of the variance in the data.\n\n\nCode\npca=PCA()\npca.fit(df)\nexp_variance=np.cumsum(pca.explained_variance_ratio_)\ncomponents=[1,2,3,4,5,6,7,8]\n\n\nplt.plot(components,exp_variance)\nplt.ylabel('Cumulative Proportion Variance Explained')\nplt.xlabel('Principal Component')\nplt.show()\n\n\n\n\n\nBased on the graph, going beyond four principal components does not result in a huge increase in the amount of the variance that is explained by the components. Therefore, I reduced the feature data to four components.\n\n\nCode\npca=PCA(n_components=4)\npca.fit(df)\nprint(pca.components_)\ndata_pca1=pca.transform(df)\ndata_pca1=pd.DataFrame(data_pca1,columns=['A','B','C','D'])\n\n\n[[ 0.01540233  0.00178136  0.2858614   0.33675383  0.23802378  0.67930777\n   0.39066758  0.36592922]\n [ 0.00996335  0.00628949  0.28774976  0.56100297  0.35014151 -0.64675638\n   0.24591146 -0.03117027]\n [-0.04444102 -0.02736794 -0.0379361  -0.30857632  0.04736612 -0.31303529\n  -0.02653487  0.89424759]\n [-0.04554206 -0.04209536 -0.20350721 -0.33429325 -0.161209   -0.12598362\n   0.8845323  -0.13685446]]\n\n\nI reduced the dimensions of the data to four components and created a new data frame with the four components labeled a A, B, C, D.\nTo visualize the components, I plotted each pair of the four features against each other in scatterplots shown below.\n\n\nCode\npca_adj_df=data_pca1\npca_adj_df['playoff']=label\npca_adj_df.replace(('Y', 'N'), (1, 0), inplace=True)\n\nfig, axs=plt.subplots(2,3)\n\naxs[0,0].scatter(pca_adj_df['A'], pca_adj_df['B'], c=pca_adj_df['playoff'], cmap='coolwarm')\n\naxs[0,1].scatter(pca_adj_df['A'], pca_adj_df['C'], c=pca_adj_df['playoff'], cmap='coolwarm')\n\naxs[0,2].scatter(pca_adj_df['A'], pca_adj_df['D'], c=pca_adj_df['playoff'], cmap='coolwarm')\n\naxs[1,0].scatter(pca_adj_df['B'], pca_adj_df['C'], c=pca_adj_df['playoff'], cmap='coolwarm')\n\naxs[1,1].scatter(pca_adj_df['B'], pca_adj_df['D'], c=pca_adj_df['playoff'], cmap='coolwarm')\n\naxs[1,2].scatter(pca_adj_df['C'], pca_adj_df['D'], c=pca_adj_df['playoff'], cmap='coolwarm')\nfor ax in axs.flat:\n    ax.set(xlabel='PCA 1', ylabel='PCA  2')\nfor ax in axs.flat:\n    ax.label_outer()\nfig.suptitle('Comparison of PCA Components')\nplt.show()\n\n\n\n\n\nAll of the features plot against each other do show a significant amount of variance between each other which is what we are looking for with dimensionality reduction."
  },
  {
    "objectID": "Clustering.html#introduction",
    "href": "Clustering.html#introduction",
    "title": "Clustering",
    "section": "",
    "text": "In this part of the project I will be clustering a data point for each NHL team from the 2022 season based on the 7 features in the dataset that I have been previously working with. With the clustering analysis, I aim to elucidate the number of categories of teams based on quality of play in the season. There are traditionally thought to be 3 main types of teams: playoff teams, teams that are in contention and miss the playoffs, and teams that are out of playoff contention most of the season. Finding the ideal number of clusters and seeing if these clusters align with the categories of teams will give insight as to how accurate this assertion is."
  },
  {
    "objectID": "decision-trees.html#methods",
    "href": "decision-trees.html#methods",
    "title": "Decision Trees",
    "section": "",
    "text": "Decision trees are a supervised learning model which can be used for either classification or regression. A decision tree is a model that follows a certain sequence of rules which form many different paths creating a branching structure that can be followed down from the top to the bottom to classify a data point with certain feautures into one of the available labels. The process starts with a root node that is alone at the top of the branching structure. At the root node, a rule is defined that will provide one of two outcomes for any given data point based on its features. Each of the two outcomes corresponds to one of the branching paths. Each path will lead to another node that will again have a defined rule that splits the data into one of two pathways. This process continues for any number of nodes until a terminal node is reached where one of the two outcomes from the rule is assigning a value of the target variable.\nDecision tree models are trained on a labelled set of training data and without defined hyperparameters they will split all of the given training data into nodes until all of the terminal nodes are pure meaning all of the data points at that node have the same value of the target variable. If there is a mix of different target variables values than the node is called impure. When the decision tree is being formed, a good split of the data is one where the two nodes the data is split into have purer class distribution. The purity of the node can be measured using a metric called the Gini Impurity score. Gini Impurity takes on a value between 0 and 0.5. The Gini Impurity for the split is calculated by taking a weighted average of the Gini Impurities of the datasets at the leaves. The best split is determined by calculating the gain for each split option. The gain is caluclated by subtracting the Gini Impurity after the data is split from the Gini impurity of all of the data before splitting.\nDecision trees are relatively prone to overfitting, but pruning by hyperparameter tuning can be used to resolve this issue. Pruning reduces the size of the tree through several different methods reducing the complexity of the tree. One hyperparameter that can be adjusted is the maximum number of layers. The maximum number of the layers is the number of nodes the tree can have before it will stop being split. Another one is setting a minimum number of samples that is required to split a node. Both of these methods of pruning help combat overfitting to improve predictive accuracy by removing sections of the trees that are less important of redundant."
  },
  {
    "objectID": "exploratory_data_analysis.html#conclusions-and-hypothesis",
    "href": "exploratory_data_analysis.html#conclusions-and-hypothesis",
    "title": "Exploratory Data Analysis",
    "section": "Conclusions and Hypothesis",
    "text": "Conclusions and Hypothesis\nBased on the exploration of this data, we can conclude that power play success (in several different metrics) does relate to team success so evaluating teams success and quality during power plays should reveal predictors for regular season success. Additionally we learned that players don’t necessarily have equally strong metrics in regular play and power play minutes. Combining these two results, I am hypothesizing that using metrics from power play data can provide different results when predicting the level of success a team will have at the conclusion of the regular season."
  },
  {
    "objectID": "data-gathering.html",
    "href": "data-gathering.html",
    "title": "Data Gathering",
    "section": "",
    "text": "Overview\nIn this project, I use three different sets of data. These include data sets are the following: 1. Team data by season 2. Individual player data in 2022 3. Team records data\n\n\nTeam Data by Season\nThe team data by season data comes from the website moneypuck.com which is one of the foremost hockey analytics websites. The website tracks live data from current games and the current season and also provides data on hockey betting, playoff odds, and league wide standings. The team data by season can be downloaded from the data tab on the website as a csv file.\nThe data is split into different files for each season. In this portfolio I use data going back five seasons to the 2018-2019 season. I excluded the dataset for the 2019-2020 season because it was cut short due to COVID and then continued at a later date in a different format.\nThe data includes five rows for each of the 32 NHL teams; one row for 5 on 5 play, one row for 5 on 4 play, one row for 4 on 5 play, one row for any other situation (such as 5 on 3, or 3 on 3 in overtime), and one row that includes data from all game situations. There are 100 variables measured so in the data cleaning process I will have to reduce these to the most relevant for my data science question.\n\n\nIndividual Player Data\nThe individual player data from 2022 also came from the moneypuck.com website and can be downloaded as a csv file from the tab on the website. The data can be downloaded by season so I took data from the 2022-2023 season. For the individual player data I took data for only one season because there are far more observations of individual players in a season than the 32 NHL teams and there are slight rule changes and variations in the game across seasons so it is better to only compare within a season when possible.\nSimilar to the team data, there are 100 variables recorded which will have to be reduced in the data cleaning process.\n\n\nTeam Record Data\nThe team record data comes from hockey-reference.com. hockey-reference.com contains traditional data on the NHL going back to the 1917-1918 season. The data available in the website includes data for teams and players by career and season as well as record holder data in many categories. The dataset I downloaded includes many traditional hockey statistics including league standing points, power play percentage, and power play goals for each team during the 2022-2023 season. In this portfolio, these three variables will be used from this dataset so other information will have to be removed during the data cleaning process."
  },
  {
    "objectID": "data-cleaning.html",
    "href": "data-cleaning.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "Code\n#load data\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\ndata &lt;- read.csv(\"../data/00-raw-data/skaters.csv\")\n\n#check for missing values\nsum(is.na(data))\n\n\n[1] 0\n\n\nCode\n#remove unnecessary columns and rename columns\ndata &lt;- subset(data, select=c(playerId,season,name,team,position,situation,games_played,icetime,shifts,iceTimeRank,onIce_xGoalsPercentage,onIce_corsiPercentage,I_F_shotAttempts,I_F_points,I_F_goals,I_F_faceOffsWon,I_F_takeaways,I_F_giveaways,I_F_hits))\ncolnames(data) &lt;- c(\"ID\",\"season\",\"name\",\"team\",\"position\",\"situation\",\"games_played\",\"icetime\",\"shifts\",\"iceTimeRank\",\"expectedGoals\",\"corsi\",\"shotAttempts\",\"points\",\"goals\",\"faceoffsWon\",\"takeways\",\"giveaways\",\"hits\")\n\n#remove players who played in fewer than half of the games in the season\ndata &lt;- filter(data, games_played&gt;=41)\n\n#separate dataframes for regular play, power plays, and penalty kills\nregular_player_data &lt;- filter(data, situation==\"5on5\")\npp_player_data &lt;- filter(data, situation==\"5on4\")\n\n#save cleaned data\nwrite.csv(regular_player_data, \"../data/01-modified-data/skaters_clean.csv\")\nwrite.csv(pp_player_data, \"../data/01-modified-data/skaters_pp.csv\")\n\n\nThe first data set that I used in this portfolio was the data on players in the 2022-2023 season. I imported the data as a csv file to a data frame in R. The first step I took in the data cleaning process was checking of any missing values. This data set came relatively clean so there were not any missing values.\nThe data frame did however start with 154 total columns which is too large for the scope of this project so I selected 19 columns to keep in the data set including identification information and 9 variables that are metrics of how well the player plays. I also had to rename the columns to simpler names.\nThe next step I took was removing players who were not regular starters on their teams. This was done by filtering by games played so only players who have played in at least half of the games in the season are included.\nFinally, I separated the data into data on players during power plays and data on players during regular 5 on 5 play and exported these separated data frames to new CSV files."
  },
  {
    "objectID": "data-cleaning.html#player-data-cleaning",
    "href": "data-cleaning.html#player-data-cleaning",
    "title": "Data Cleaning",
    "section": "",
    "text": "Code\n#load data\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\ndata &lt;- read.csv(\"../data/00-raw-data/skaters.csv\")\n\n#check for missing values\nsum(is.na(data))\n\n\n[1] 0\n\n\nCode\n#remove unnecessary columns and rename columns\ndata &lt;- subset(data, select=c(playerId,season,name,team,position,situation,games_played,icetime,shifts,iceTimeRank,onIce_xGoalsPercentage,onIce_corsiPercentage,I_F_shotAttempts,I_F_points,I_F_goals,I_F_faceOffsWon,I_F_takeaways,I_F_giveaways,I_F_hits))\ncolnames(data) &lt;- c(\"ID\",\"season\",\"name\",\"team\",\"position\",\"situation\",\"games_played\",\"icetime\",\"shifts\",\"iceTimeRank\",\"expectedGoals\",\"corsi\",\"shotAttempts\",\"points\",\"goals\",\"faceoffsWon\",\"takeways\",\"giveaways\",\"hits\")\n\n#remove players who played in fewer than half of the games in the season\ndata &lt;- filter(data, games_played&gt;=41)\n\n#separate dataframes for regular play, power plays, and penalty kills\nregular_player_data &lt;- filter(data, situation==\"5on5\")\npp_player_data &lt;- filter(data, situation==\"5on4\")\n\n#save cleaned data\nwrite.csv(regular_player_data, \"../data/01-modified-data/skaters_clean.csv\")\nwrite.csv(pp_player_data, \"../data/01-modified-data/skaters_pp.csv\")\n\n\nThe first data set that I used in this portfolio was the data on players in the 2022-2023 season. I imported the data as a csv file to a data frame in R. The first step I took in the data cleaning process was checking of any missing values. This data set came relatively clean so there were not any missing values.\nThe data frame did however start with 154 total columns which is too large for the scope of this project so I selected 19 columns to keep in the data set including identification information and 9 variables that are metrics of how well the player plays. I also had to rename the columns to simpler names.\nThe next step I took was removing players who were not regular starters on their teams. This was done by filtering by games played so only players who have played in at least half of the games in the season are included.\nFinally, I separated the data into data on players during power plays and data on players during regular 5 on 5 play and exported these separated data frames to new CSV files."
  },
  {
    "objectID": "data-cleaning.html#team-record-data",
    "href": "data-cleaning.html#team-record-data",
    "title": "Data Cleaning",
    "section": "Team Record Data",
    "text": "Team Record Data\n\n\nCode\n#load data\nlibrary(tidyverse)\ndata &lt;- read.csv(\"../data/00-raw-data/team_records.csv\")\n\n#check for missing values\nsum(is.na(data))\n\n\n[1] 0\n\n\nCode\n#remove unnecessary columns and change column names\ndata &lt;- subset(data, select=c(X,PTS, PP, PPO, PP., PPA, PPOA, PK.))\ncolnames(data) &lt;- c(\"team\",\"points\", \"ppGoals\",\"ppOpps\",\"ppPercent\",\"pkGoalsAgainst\",\"pkOppsAgainst\",\"pkPercent\")\n\n#change team names to match other datasets\ndata$team &lt;- c(\"BOS\",\"CAR\",\"NJD\",\"VGK\",\"TOR\",\"COL\",\"EDM\",\"DAL\",\"NYR\",\"LAK\",\"MIN\",\"SEA\",\"TBL\",\"WPG\",\"CGY\",\"NYI\",\"FLA\",\"NSH\",\"BUF\",\"PIT\",\"OTT\",\"VAN\",\"STL\",\"WSH\",\"DET\",\"PHI\",\"ARI\",\"MTL\",\"SJS\",\"CHI\",\"CBJ\",\"ANA\")\n\n\n#save cleaned data\nwrite.csv(data, \"../data/01-modified-data/team_record_clean.csv\")\n\n\nThe next data set I use in the portfolio is the team record data. I first imported the CSV of the data set as a data frame in R. I then checked for missing values in the data and there were none.\nAfter that I selected the columns that will be relevant for my data science questions and renamed the columns. These columns include team name, points, power play goals, power play opportunities, power play percent, penalty kill goals against, penalty opportunities against, and penalty kill percent.\nThe team name column did not use the same abbreviations as the team name column in other data sets used in the portfolio so I adjusted the values of the columns to match.\nI then exported the clean data frame to a CSV file."
  },
  {
    "objectID": "data-cleaning.html#team-power-play-and-regular-play-data",
    "href": "data-cleaning.html#team-power-play-and-regular-play-data",
    "title": "Data Cleaning",
    "section": "Team Power Play and Regular Play Data",
    "text": "Team Power Play and Regular Play Data\n\nPower Play Data\n\n\nCode\n#load data\ndata2022 &lt;- read.csv(\"../data/00-raw-data/team_data_2022.csv\")\ndata2021 &lt;- read.csv(\"../data/00-raw-data/team_data_2021.csv\")\ndata2020 &lt;- read.csv(\"../data/00-raw-data/team_data_2020.csv\")\ndata2018 &lt;- read.csv(\"../data/00-raw-data/team_data_2018.csv\")\nlibrary(tidyverse)\n\n#check for missing values\nsum(is.na(data2022))\n\n\n[1] 0\n\n\nCode\nsum(is.na(data2021))\n\n\n[1] 0\n\n\nCode\nsum(is.na(data2020))\n\n\n[1] 0\n\n\nCode\nsum(is.na(data2018))\n\n\n[1] 0\n\n\nCode\n#remove unnecessary columns and change column names\ndata2022 &lt;- subset(data2022, select=c(team,situation, xGoalsPercentage,corsiPercentage,shotAttemptsFor,goalsFor,faceOffsWonFor,takeawaysFor,giveawaysFor,hitsFor,team,situation))\ndata2021 &lt;- subset(data2021, select=c(team,situation, xGoalsPercentage,corsiPercentage,shotAttemptsFor,goalsFor,faceOffsWonFor,takeawaysFor,giveawaysFor,hitsFor,team,situation))\ndata2020 &lt;- subset(data2020, select=c(team,situation, xGoalsPercentage,corsiPercentage,shotAttemptsFor,goalsFor,faceOffsWonFor,takeawaysFor,giveawaysFor,hitsFor,team,situation))\ndata2018 &lt;- subset(data2018, select=c(team,situation, xGoalsPercentage,corsiPercentage,shotAttemptsFor,goalsFor,faceOffsWonFor,takeawaysFor,giveawaysFor,hitsFor,team,situation))\n\n#filter non powerplay data\ndata2022&lt;- data2022 %&gt;% filter(situation==\"5on4\")\ndata2021&lt;- data2021 %&gt;% filter(situation==\"5on4\")\ndata2020&lt;- data2020 %&gt;% filter(situation==\"5on4\")\ndata2018&lt;- data2018 %&gt;% filter(situation==\"5on4\")\n\n#add variable for playoff teams\nplayoff_teams_2018 &lt;- c(\"T.B\",\"BOS\",\"TOR\",\"WSH\",\"NYI\",\"PIT\",\"CAR\",\"CBJ\",\"NSH\",\"WPG\",\"STL\",\"DAL\",\"COL\",\"CGY\",\"S.J\",\"VGK\")\nplayoff_teams_2020 &lt;- c(\"CAR\",\"FLA\",\"T.B\",\"NSH\",\"PIT\",\"WSH\",\"BOS\",\"NYI\",\"TOR\",\"EDM\",\"WPG\",\"MTL\",\"COL\",\"VGK\",\"MIN\",\"STL\")\nplayoff_teams_2021 &lt;- c(\"FLA\",\"TOR\",\"TBL\",\"BOS\",\"CAR\",\"NYR\",\"PIT\",\"WSH\",\"COL\",\"MIN\",\"STL\",\"DAL\",\"NSH\",\"CGY\",\"EDM\",\"LAK\")\nplayoff_teams_2022 &lt;- c(\"BOS\",\"TOR\",\"TBL\",\"FLA\",\"CAR\",\"NJD\",\"NYR\",\"NYI\",\"COL\",\"DAL\",\"MIN\",\"WPG\",\"VGK\",\"EDM\",\"LAK\",\"SEA\")\n\ndata2022$playoff &lt;- NA\ndata2021$playoff &lt;- NA\ndata2020$playoff &lt;- NA\ndata2018$playoff &lt;- NA\n\ndata2022&lt;-data2022 %&gt;% mutate(playoff = ifelse(team %in% playoff_teams_2022,1,0))\ndata2021&lt;-data2021 %&gt;% mutate(playoff = ifelse(team %in% playoff_teams_2021,1,0))\ndata2020&lt;-data2020 %&gt;% mutate(playoff = ifelse(team %in% playoff_teams_2020,1,0))\ndata2018&lt;-data2018 %&gt;% mutate(playoff = ifelse(team %in% playoff_teams_2018,1,0))\n\n\n\n#combine datasets\ndata &lt;- rbind(data2022, data2021, data2020, data2018)\n\n#remove non-numerical data\ndata &lt;- subset(data, select=-c(team,situation,team.1,situation.1))\n\n#normalize numerical data\ndata&lt;-mutate(data, xGoalsPercentage=xGoalsPercentage/mean(xGoalsPercentage), corsiPercentage=corsiPercentage/mean(corsiPercentage), shotAttemptsFor=shotAttemptsFor/mean(shotAttemptsFor), goalsFor=goalsFor/mean(goalsFor), faceOffsWonFor=faceOffsWonFor/mean(faceOffsWonFor), takeawaysFor=takeawaysFor/mean(takeawaysFor),giveawaysFor=giveawaysFor/mean(giveawaysFor),hitsFor=hitsFor/mean(hitsFor))\n\n#save cleaned data\nwrite.csv(data, \"../data/01-modified-data/team_pp_data_clean.csv\")\n\n\n\n\nRegular Play Data\n\n\nCode\n#load data\ndata2022 &lt;- read.csv(\"../data/00-raw-data/team_data_2022.csv\")\ndata2021 &lt;- read.csv(\"../data/00-raw-data/team_data_2021.csv\")\ndata2020 &lt;- read.csv(\"../data/00-raw-data/team_data_2020.csv\")\ndata2018 &lt;- read.csv(\"../data/00-raw-data/team_data_2018.csv\")\nlibrary(tidyverse)\n\n#check for missing values\nsum(is.na(data2022))\n\n\n[1] 0\n\n\nCode\nsum(is.na(data2021))\n\n\n[1] 0\n\n\nCode\nsum(is.na(data2020))\n\n\n[1] 0\n\n\nCode\nsum(is.na(data2018))\n\n\n[1] 0\n\n\nCode\n#remove unnecessary columns and change column names\ndata2022 &lt;- subset(data2022, select=c(team,situation, xGoalsPercentage,corsiPercentage,shotAttemptsFor,goalsFor,faceOffsWonFor,takeawaysFor,giveawaysFor,hitsFor,team,situation))\ndata2021 &lt;- subset(data2021, select=c(team,situation, xGoalsPercentage,corsiPercentage,shotAttemptsFor,goalsFor,faceOffsWonFor,takeawaysFor,giveawaysFor,hitsFor,team,situation))\ndata2020 &lt;- subset(data2020, select=c(team,situation, xGoalsPercentage,corsiPercentage,shotAttemptsFor,goalsFor,faceOffsWonFor,takeawaysFor,giveawaysFor,hitsFor,team,situation))\ndata2018 &lt;- subset(data2018, select=c(team,situation, xGoalsPercentage,corsiPercentage,shotAttemptsFor,goalsFor,faceOffsWonFor,takeawaysFor,giveawaysFor,hitsFor,team,situation))\n\n#filter non regular play data\ndata2022&lt;- data2022 %&gt;% filter(situation==\"5on5\")\ndata2021&lt;- data2021 %&gt;% filter(situation==\"5on5\")\ndata2020&lt;- data2020 %&gt;% filter(situation==\"5on5\")\ndata2018&lt;- data2018 %&gt;% filter(situation==\"5on5\")\n\n#add variable for playoff teams\nplayoff_teams_2018 &lt;- c(\"T.B\",\"BOS\",\"TOR\",\"WSH\",\"NYI\",\"PIT\",\"CAR\",\"CBJ\",\"NSH\",\"WPG\",\"STL\",\"DAL\",\"COL\",\"CGY\",\"S.J\",\"VGK\")\nplayoff_teams_2020 &lt;- c(\"CAR\",\"FLA\",\"T.B\",\"NSH\",\"PIT\",\"WSH\",\"BOS\",\"NYI\",\"TOR\",\"EDM\",\"WPG\",\"MTL\",\"COL\",\"VGK\",\"MIN\",\"STL\")\nplayoff_teams_2021 &lt;- c(\"FLA\",\"TOR\",\"TBL\",\"BOS\",\"CAR\",\"NYR\",\"PIT\",\"WSH\",\"COL\",\"MIN\",\"STL\",\"DAL\",\"NSH\",\"CGY\",\"EDM\",\"LAK\")\nplayoff_teams_2022 &lt;- c(\"BOS\",\"TOR\",\"TBL\",\"FLA\",\"CAR\",\"NJD\",\"NYR\",\"NYI\",\"COL\",\"DAL\",\"MIN\",\"WPG\",\"VGK\",\"EDM\",\"LAK\",\"SEA\")\n\ndata2022$playoff &lt;- NA\ndata2021$playoff &lt;- NA\ndata2020$playoff &lt;- NA\ndata2018$playoff &lt;- NA\n\ndata2022&lt;-data2022 %&gt;% mutate(playoff = ifelse(team %in% playoff_teams_2022,1,0))\ndata2021&lt;-data2021 %&gt;% mutate(playoff = ifelse(team %in% playoff_teams_2021,1,0))\ndata2020&lt;-data2020 %&gt;% mutate(playoff = ifelse(team %in% playoff_teams_2020,1,0))\ndata2018&lt;-data2018 %&gt;% mutate(playoff = ifelse(team %in% playoff_teams_2018,1,0))\n\n\n\n#combine datasets\ndata &lt;- rbind(data2022, data2021, data2020, data2018)\n\n#remove non-numerical data\ndata &lt;- subset(data, select=-c(team,situation,team.1,situation.1))\n\n#normalize numerical data\ndata&lt;-mutate(data, xGoalsPercentage=xGoalsPercentage/mean(xGoalsPercentage), corsiPercentage=corsiPercentage/mean(corsiPercentage), shotAttemptsFor=shotAttemptsFor/mean(shotAttemptsFor), goalsFor=goalsFor/mean(goalsFor), faceOffsWonFor=faceOffsWonFor/mean(faceOffsWonFor), takeawaysFor=takeawaysFor/mean(takeawaysFor),giveawaysFor=giveawaysFor/mean(giveawaysFor),hitsFor=hitsFor/mean(hitsFor))\n\n#save cleaned data\nwrite.csv(data, \"../data/01-modified-data/team_regular_data_clean.csv\")\n\n\nThe power play team data and regular play team data had identical cleaning processes. They both started with importing team data for each of the four included seasons and checking for missing values. There were no missing values in these data sets.\nNext, I selected the relevant columns for each of the four data sets and renamed them, then filtered the data sets by either 5 on 5 play of 5 on 4 play. I had to add a label variable for whether or not each team made the playoffs in that season so I added an empty column to each data set and created a variable that included the names of the playoff teams in each season. I was then able to mutate the playoff variable to assign each team 0 (meaning they did not make the playoffs) or 1 (meaning the did make the playoffs).\nWith the new playoff variable, the data sets for the four years were combined into one and non-numerical data was removed. After removing the non-numerical data, the numerical data was normalized.\nFollowing this, the data was prepped to be used in a Naive Bayes model so I exported each of the two data frames to CSV files."
  },
  {
    "objectID": "data-cleaning.html#team-power-play-data-with-point-brackets",
    "href": "data-cleaning.html#team-power-play-data-with-point-brackets",
    "title": "Data Cleaning",
    "section": "Team Power Play Data with Point Brackets",
    "text": "Team Power Play Data with Point Brackets\nThe next data set I cleaned and used was the power play data with the labels being point brackets instead of a binary playoff variable assignment.\n\n\nCode\n#load data\ndata2022 &lt;- read.csv(\"../data/00-raw-data/team_data_2022.csv\")\nlibrary(tidyverse)\n\n#check for missing values\nsum(is.na(data2022))\n\n\n[1] 0\n\n\nCode\n#remove unnecessary columns and change column names\ndata2022 &lt;- subset(data2022, select=c(team,situation, xGoalsPercentage,corsiPercentage,shotAttemptsFor,xGoalsFor,faceOffsWonFor,takeawaysFor,giveawaysFor,hitsFor,team,situation))\n\n#filter non powerplay data\ndata2022&lt;- data2022 %&gt;% filter(situation==\"5on4\")\n\n\n#add variable for points bracket\npoints_bracket &lt;- c(\"mid_tier\",\"over100\",\"under80\",\"under80\",\"mid_tier\",\"mid_tier\",\"over100\",\"mid_tier\",\"mid_tier\",\"over100\",\"mid_tier\",\"under80\",\"mid_tier\",\"mid_tier\",\"over100\",\"over100\",\"over100\",\"over100\",\"under80\",\"over100\",\"over100\",\"under80\",\"under80\",\"under80\",\"over100\",\"mid_tier\",\"mid_tier\",\"over100\",\"over100\",\"mid_tier\",\"mid_tier\",\"mid_tier\")\ndata &lt;- data2022 %&gt;% mutate(points_cat=points_bracket)\n\n\n\n#remove non-numerical data\ndata &lt;- subset(data, select=-c(team,situation,team.1,situation.1))\n\n#normalize numerical data\ndata&lt;-mutate(data, xGoalsPercentage=xGoalsPercentage/mean(xGoalsPercentage), corsiPercentage=corsiPercentage/mean(corsiPercentage), shotAttemptsFor=shotAttemptsFor/mean(shotAttemptsFor), xGoalsFor=xGoalsFor/mean(xGoalsFor), faceOffsWonFor=faceOffsWonFor/mean(faceOffsWonFor), takeawaysFor=takeawaysFor/mean(takeawaysFor),giveawaysFor=giveawaysFor/mean(giveawaysFor),hitsFor=hitsFor/mean(hitsFor))\n\n#save cleaned data\nwrite.csv(data, \"../data/01-modified-data/team_pp_data_point_bracket_clean.csv\")\n\n\nI imported the 2022 team data file then checked for missing values. I then selected the relevant columns and filtered out the non-power play data. After that, I added a label variable called points bracket that assigned each team a range that they fell in of season standing points.\nTo prep for clustering, I removed the non-numerical data and normalized the numerical data, then exported the cleaned data frame to a CSV file."
  },
  {
    "objectID": "Conclusions.html",
    "href": "Conclusions.html",
    "title": "Conclusions",
    "section": "",
    "text": "The power play is universally considered one of the most important parts of the game of hockey by most hockey fans. However, in my background research on the topic of analytical approaches to understanding and predicting hockey results I found that the action that occurs during power play minutes in NHL games is understudied and underutilized. In this portfolio I attempted to answer some questions about what we can learn from data from power plays and how this can be applied to a greater understanding of the quality of NHL teams. In this section I will summarize the results from the many methods I applied to answering these questions.\n\n\nFrom performing preliminary data analysis, I learned that most of the traditional and simple modern hockey performance metrics have higher values for the same player in power play situations compared to regular play situations. Consequently, I concluded that power play time is very different play from regular situation play and this conclusion was corroborated by the demonstration that metrics during power play time are not necessarily correlated with that same metric during regular play time for a given player.\nThe other major conclusion from the data exploration process was that team success measured by final standings points does appear to be correlated with power play success (measured in both total power play goals scored and power play success rate). This conclusion supports the idea that evaluating power play data can be very useful in creating a better data backed understanding of what a good NHL team is.\n\n\n\nFollowing the exploratory data analysis process, I used a Naive Bayes classification machine learning model to predict whether or not a team would make the playoffs based on team expected goals percentage, Corsi, goals for, hits, takeaways, faceoffs won, giveaways, and shot attempts.\nConstructiing a model with both regular and power play data using both feature selection and no feature selection led to the following conclusions:\nThe models that used all eight features had a higher accuracy and precision when using power play data to predict playoff berths than regular data so team play during a power play may be more predictive of playoff success than regular situation play.\nWhen using feature selection, the model that used regular play data had slightly higher accuracy but lower precision than the model that used power play data. Additionally, the power play data model did not use as many features to achieve these metrics. Therefore, using power play data with feature selection may create the best model when considering computational efficiency and preventing overfitting when applying to a large data set.\n\n\n\nUsing clustering models on the team data with the features mentioned above revealed that there are three clear groups of teams that are more similar to each other than they are to other teams. These groupings did not line up with the labels I used for ranges of final standing points, but they did reveal that the bottom teams in the league standings that earned fewer than 80 points are more similar to each other based on these metrics during power plays than they are to any other teams in different point ranges. However, middle tier teams and top tier teams based on standings points may be more similar to each other than it appears based on standings.\n\n\n\nCreating and tuning a decision tree that predicts whether a team will make the playoffs based on metrics from power play data revealed that a major factor that separates playoff teams from non-playoff teams is team expected goal percentage on the power play. This makes sense when considering the importance of scoring goals on power plays that we determined during the exploratory data analysis process.\nThe other conclusion gathered from the decision tree fitting process was that it is easier to predict from these metrics which teams will not make the playoffs than it is to properly classify teams that do not make the playoffs. This is consistent with the conclusion from the clustering tab that bottom tier teams are very similar to each other, but at the middle and top of the pack their is a great deal of parity."
  },
  {
    "objectID": "Conclusions.html#data-exploration",
    "href": "Conclusions.html#data-exploration",
    "title": "Conclusions",
    "section": "",
    "text": "From performing preliminary data analysis, I learned that most of the traditional and simple modern hockey performance metrics have higher values for the same player in power play situations compared to regular play situations. Consequently, I concluded that power play time is very different play from regular situation play and this conclusion was corroborated by the demonstration that metrics during power play time are not necessarily correlated with that same metric during regular play time for a given player.\nThe other major conclusion from the data exploration process was that team success measured by final standings points does appear to be correlated with power play success (measured in both total power play goals scored and power play success rate). This conclusion supports the idea that evaluating power play data can be very useful in creating a better data backed understanding of what a good NHL team is."
  },
  {
    "objectID": "Conclusions.html#naive-bayes-model",
    "href": "Conclusions.html#naive-bayes-model",
    "title": "Conclusions",
    "section": "",
    "text": "Following the exploratory data analysis process, I used a Naive Bayes classification machine learning model to predict whether or not a team would make the playoffs based on team expected goals percentage, Corsi, goals for, hits, takeaways, faceoffs won, giveaways, and shot attempts.\nConstructiing a model with both regular and power play data using both feature selection and no feature selection led to the following conclusions:\nThe models that used all eight features had a higher accuracy and precision when using power play data to predict playoff berths than regular data so team play during a power play may be more predictive of playoff success than regular situation play.\nWhen using feature selection, the model that used regular play data had slightly higher accuracy but lower precision than the model that used power play data. Additionally, the power play data model did not use as many features to achieve these metrics. Therefore, using power play data with feature selection may create the best model when considering computational efficiency and preventing overfitting when applying to a large data set."
  },
  {
    "objectID": "Conclusions.html#clustering",
    "href": "Conclusions.html#clustering",
    "title": "Conclusions",
    "section": "",
    "text": "Using clustering models on the team data with the features mentioned above revealed that there are three clear groups of teams that are more similar to each other than they are to other teams. These groupings did not line up with the labels I used for ranges of final standing points, but they did reveal that the bottom teams in the league standings that earned fewer than 80 points are more similar to each other based on these metrics during power plays than they are to any other teams in different point ranges. However, middle tier teams and top tier teams based on standings points may be more similar to each other than it appears based on standings."
  },
  {
    "objectID": "Conclusions.html#decision-trees",
    "href": "Conclusions.html#decision-trees",
    "title": "Conclusions",
    "section": "",
    "text": "Creating and tuning a decision tree that predicts whether a team will make the playoffs based on metrics from power play data revealed that a major factor that separates playoff teams from non-playoff teams is team expected goal percentage on the power play. This makes sense when considering the importance of scoring goals on power plays that we determined during the exploratory data analysis process.\nThe other conclusion gathered from the decision tree fitting process was that it is easier to predict from these metrics which teams will not make the playoffs than it is to properly classify teams that do not make the playoffs. This is consistent with the conclusion from the clustering tab that bottom tier teams are very similar to each other, but at the middle and top of the pack their is a great deal of parity."
  },
  {
    "objectID": "introduction.html#references",
    "href": "introduction.html#references",
    "title": "Introduction",
    "section": "References",
    "text": "References\n(Nandakumar and Jensen 2019)\n(Macdonald 2012)"
  },
  {
    "objectID": "naive_bayes.html#references",
    "href": "naive_bayes.html#references",
    "title": "Introduction to Naive Bayes",
    "section": "References",
    "text": "References\nDSAN 5000 Lecture Content (Hickman 2023)"
  },
  {
    "objectID": "data-cleaning.html#references",
    "href": "data-cleaning.html#references",
    "title": "Data Cleaning",
    "section": "References",
    "text": "References\nDSAN 5000 Lecture Content (Hickman 2023)"
  },
  {
    "objectID": "dimensionality-reduction.html#references",
    "href": "dimensionality-reduction.html#references",
    "title": "Dimensionality Reduction",
    "section": "References",
    "text": "References\nDSAN 5000 Lecture Content (Hickman 2023)"
  },
  {
    "objectID": "decision-trees.html#references",
    "href": "decision-trees.html#references",
    "title": "Decision Trees",
    "section": "References",
    "text": "References\nDSAN 5000 Lecture Content (Hickman 2023)"
  },
  {
    "objectID": "exploratory_data_analysis.html#references",
    "href": "exploratory_data_analysis.html#references",
    "title": "Exploratory Data Analysis",
    "section": "References",
    "text": "References\nDSAN 5000 Lecture Content (Hickman 2023)"
  },
  {
    "objectID": "Clustering.html#references",
    "href": "Clustering.html#references",
    "title": "Clustering",
    "section": "References",
    "text": "References\nDSAN 5000 Lecture Content (Hickman 2023)"
  },
  {
    "objectID": "data-gathering.html#references",
    "href": "data-gathering.html#references",
    "title": "Overview",
    "section": "References",
    "text": "References\n\"Moneypuck.Com - Data.\" MoneyPuck.Com , moneypuck.com/. Accessed 6 Dec. 2023.\n\"2022-23 NHL Standings.\" Hockey Reference, www.hockey-reference.com/leagues/NHL_2023_standings.html. Accessed 6 Dec. 2023."
  }
]